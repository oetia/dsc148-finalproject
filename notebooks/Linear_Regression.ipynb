{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None, verbose = False):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "            \n",
    "            attributes: \n",
    "                        alpha: Learning Rate, default 1e-10\n",
    "                        num_iter: Number of Iterations to update coefficient with training data\n",
    "                        early_stop: Constant control early_stop.\n",
    "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
    "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                        \n",
    "            \n",
    "            TODO: 1. Initialize all variables needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Linear Regression'\n",
    "\n",
    "        # store the variables in object scope\n",
    "        self.alpha: float = alpha\n",
    "        self.num_iter: int = num_iter\n",
    "        self.early_stop: float = early_stop\n",
    "        self.intercept: bool = intercept\n",
    "        self.init_weight = init_weight  ### For testing correctness.\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and perform gradient descent.\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "                \n",
    "                \n",
    "                TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
    "                      3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
    "                      4. Call the gradient_descent function to train.\n",
    "        \"\"\"\n",
    "        \n",
    "        # i'm not sure what the dimensions of the input data mean\n",
    "        # since they say add a column, i'm assuming that each row is an example\n",
    "        \n",
    "        self.X = np.array(X_train)\n",
    "        self.y = np.array(y_train).reshape(-1, 1) \n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones((self.X.shape[0], 1))\n",
    "            self.X = np.hstack([ones, self.X])\n",
    "            self.log(self.X.shape)\n",
    "            self.log(self.X)\n",
    "            \n",
    "        np.random.seed(16)\n",
    "        self.coef = np.random.uniform(-1, 1, (self.X.shape[1], 1),)\n",
    "        self.log(self.coef)\n",
    "        \n",
    "        self.gradient_descent()\n",
    "        \n",
    "        # self.coef = self.init_weight #### Please change this after you get the example right.\n",
    "        \n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "            Helper function to calculate the gradient respect to coefficient.\n",
    "            \n",
    "            TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # SE Loss function. Ignoring the normalization across examples. Feels strange, but I guess that it should still work. Depending on the dataset, could approach overflow. \n",
    "        # I would rather think of this in terms of forward and backward pass\n",
    "        # The way that things are separted into two function calls where the forward pass values can't be passed into the backward pass values annoys me. But w.e. this is how the assignment is designed. \n",
    "        \n",
    "        # forward pass\n",
    "        # pred = X*coef\n",
    "        # diff = y-pred\n",
    "        # loss = sum (y-pred)**2\n",
    "       \n",
    "        # print(self.y.shape)\n",
    "        # print(self.X.shape)\n",
    "        # print(self.coef.shape)\n",
    "        \n",
    "        # backward pass (gradients) \n",
    "        d_loss = 1\n",
    "        d_diff = d_loss * 2 * (self.y - self.X @ self.coef)\n",
    "        d_pred = d_diff * -1\n",
    "        # each coefficient affects every prediction\n",
    "        # each coefficient affects a column of X. an increase in an coefficient leads to an proportionate increase in the predicted values of that column of X. \n",
    "        # d_pred = (num_examples)\n",
    "        # manual backward pass. thinking about how each value affects the subsequent values in the graph\n",
    "        d_coef = (np.asarray(d_pred) * np.asarray(self.X)).sum(axis=0)\n",
    "        \n",
    "        # doesn't make sense to have to calculate these values twice\n",
    "        # can i just keep the gradient inside the gradient_descent function\n",
    "        grad_coef = (-2 * (self.y - self.X @ self.coef) * self.X).sum(axis=0)\n",
    "        \n",
    "        print(np.isclose(d_coef, grad_coef))\n",
    "        \n",
    "        self.grad_coef = grad_coef\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Training function\n",
    "            \n",
    "            TODO: 6. Calculate the loss with current coefficients.\n",
    "                  7. Update the temp_coef with learning rate and gradient.\n",
    "                  8. Calculate the loss with temp_coef.\n",
    "                  9. Implement the self adeptive learning rate. \n",
    "                      a. If current error is less than previous error, increase learning rate by a factor 1.3. \n",
    "                         And update coef, with temp_coef.\n",
    "                      b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
    "                         Don't update coef.\n",
    "                  10. Add the loss to loss list we create.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            # forward pass\n",
    "            preds = self.X @ self.coef\n",
    "            errors = self.y - preds\n",
    "            squared_errors = (errors**2)\n",
    "            loss = squared_errors.sum()\n",
    "            \n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            # backward pass\n",
    "            dloss = 1\n",
    "            dsquared_errors = dloss * np.ones_like(squared_errors)\n",
    "            # dsquared_errors = dloss * 1\n",
    "            derrors = dsquared_errors * 2 * errors\n",
    "            dpreds = derrors * -1\n",
    "            # dcoef = (dpreds * self.X).sum(axis=0) # for a given coefficients affects a column of x's, which each affect a different prediction\n",
    "            dcoef = (dpreds * self.X).sum(axis=0).reshape(-1, 1) # for a given coefficients affects a column of x's, which each affect a different prediction\n",
    "            # print(self.coef.shape)\n",
    "            \n",
    "            # if(self.coef.shape != (2, 1)):\n",
    "            #     break\n",
    "            # break\n",
    "            \n",
    "            # print(\"test\", self.coef)\n",
    "            # print(dcoef)\n",
    "            # print(\"test1\", self.alpha * dcoef)\n",
    "            # print(\"test2\", (self.coef * self.alpha).shape)\n",
    "            # print(\"test3\", np.subtract(self.coef, self.alpha * dcoef))\n",
    "            # test1 = self.coef\n",
    "            # test2 = self.alpha * dcoef\n",
    "            # print(\"test4\", test1 - test2)\n",
    "            # print(type(self.coef), type(dcoef))\n",
    "            # break\n",
    "\n",
    "            temp_coef = self.coef - self.alpha * dcoef\n",
    "            # print(self.coef.shape, temp_coef.shape, (self.alpha * dcoef).shape)\n",
    "            # print(self.coef, temp_coef.shape)\n",
    "            temp_loss = np.sum((self.y - self.X @ temp_coef)**2) # technically double calculates, which doesn't need to be done. i'll fix this later.\n",
    "            \n",
    "            pre_error = loss\n",
    "            \n",
    "            current_error = temp_loss\n",
    "            \n",
    "            ### This is the early stop, don't modify fllowing three lines.\n",
    "            if (abs(pre_error - current_error) < self.early_stop) | (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                print(f\"EARLY STOP @ {i}\")\n",
    "                self.coef = temp_coef\n",
    "                return self\n",
    "            \n",
    "            if current_error <= pre_error:\n",
    "                self.alpha *= 1.3\n",
    "                self.coef = temp_coef\n",
    "            else:\n",
    "                self.alpha *= 0.9 # don't update coefs\n",
    "                \n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print('Iteration: ' +  str(i))\n",
    "                print('Coef: '+ str(self.coef))\n",
    "                print('Loss: ' + str(current_error))            \n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the value based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                result: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 11. Implement the prediction function\n",
    "        \"\"\"\n",
    "        result = 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            X: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                ret: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 12. Make sure add the 1's column like we did to add intercept.\n",
    "                  13. Revise the following for-loop to call ind_predict to get predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = []\n",
    "        X = np.mat(X)\n",
    "        if self.intercept:\n",
    "            ones = 0\n",
    "            X = X\n",
    "        for x in X:\n",
    "            ret.append(1)\n",
    "        return ret\n",
    "        \n",
    "    def log(self, str):\n",
    "        if self.verbose == True:\n",
    "            print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() +  20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.55341784]\n",
      " [ 0.04632668]]\n",
      "Loss: 1.0512540537247319e+27\n",
      "Iteration: 10000\n",
      "Coef: [[-0.42121603]\n",
      " [30.03076879]]\n",
      "Loss: 20946.45307079826\n",
      "Iteration: 20000\n",
      "Coef: [[-0.33277167]\n",
      " [30.03048674]]\n",
      "Loss: 20764.404443420626\n",
      "Iteration: 30000\n",
      "Coef: [[-0.24469976]\n",
      " [30.03048024]]\n",
      "Loss: 20584.9381638687\n",
      "Iteration: 40000\n",
      "Coef: [[-0.15701385]\n",
      " [30.03021484]]\n",
      "Loss: 20406.880884395716\n",
      "Iteration: 50000\n",
      "Coef: [[-0.069677  ]\n",
      " [30.03008841]]\n",
      "Loss: 20230.36933475199\n",
      "Iteration: 60000\n",
      "Coef: [[1.72549084e-02]\n",
      " [3.00300993e+01]]\n",
      "Loss: 20056.33541968811\n",
      "Iteration: 70000\n",
      "Coef: [[ 0.10380399]\n",
      " [30.02982407]]\n",
      "Loss: 19882.431533466686\n",
      "Iteration: 80000\n",
      "Coef: [[ 0.18998496]\n",
      " [30.0298398 ]]\n",
      "Loss: 19710.67532541356\n",
      "Iteration: 90000\n",
      "Coef: [[ 0.2757859 ]\n",
      " [30.02956728]]\n",
      "Loss: 19539.945552369667\n",
      "Iteration: 100000\n",
      "Coef: [[ 0.3612478 ]\n",
      " [30.02944095]]\n",
      "Loss: 19370.95718696334\n",
      "Iteration: 110000\n",
      "Coef: [[ 0.44631426]\n",
      " [30.02945397]]\n",
      "Loss: 19204.329388099533\n",
      "Iteration: 120000\n",
      "Coef: [[ 0.53100543]\n",
      " [30.02918064]]\n",
      "Loss: 19037.842459653075\n",
      "Iteration: 130000\n",
      "Coef: [[ 0.61533303]\n",
      " [30.02919089]]\n",
      "Loss: 18873.195205826414\n",
      "Iteration: 140000\n",
      "Coef: [[ 0.69929212]\n",
      " [30.02893688]]\n",
      "Loss: 18709.808278871482\n",
      "Iteration: 150000\n",
      "Coef: [[ 0.7829227 ]\n",
      " [30.02879866]]\n",
      "Loss: 18548.125912220745\n",
      "Iteration: 160000\n",
      "Coef: [[ 0.86615989]\n",
      " [30.02882075]]\n",
      "Loss: 18388.46956807662\n",
      "Iteration: 170000\n",
      "Coef: [[ 0.9490332 ]\n",
      " [30.02855319]]\n",
      "Loss: 18229.094411618546\n",
      "Iteration: 180000\n",
      "Coef: [[ 1.03155071]\n",
      " [30.02856687]]\n",
      "Loss: 18071.46673008374\n",
      "Iteration: 190000\n",
      "Coef: [[ 1.1137074 ]\n",
      " [30.02831084]]\n",
      "Loss: 17915.02509714345\n",
      "Iteration: 200000\n",
      "Coef: [[ 1.19553953]\n",
      " [30.02818581]]\n",
      "Loss: 17760.121596956586\n",
      "Iteration: 210000\n",
      "Coef: [[ 1.27699018]\n",
      " [30.02819695]]\n",
      "Loss: 17607.16117941675\n",
      "Iteration: 220000\n",
      "Coef: [[ 1.35808461]\n",
      " [30.02794272]]\n",
      "Loss: 17454.64061416738\n",
      "Iteration: 230000\n",
      "Coef: [[ 1.43883093]\n",
      " [30.02795224]]\n",
      "Loss: 17303.736226649144\n",
      "Iteration: 240000\n",
      "Coef: [[ 1.51922397]\n",
      " [30.02770181]]\n",
      "Loss: 17153.97145601885\n",
      "Iteration: 250000\n",
      "Coef: [[ 1.59929916]\n",
      " [30.02757601]]\n",
      "Loss: 17005.676545790324\n",
      "Iteration: 260000\n",
      "Coef: [[ 1.67900119]\n",
      " [30.02760031]]\n",
      "Loss: 16859.40608571068\n",
      "Iteration: 270000\n",
      "Coef: [[ 1.7583519 ]\n",
      " [30.02734617]]\n",
      "Loss: 16713.079064636855\n",
      "Iteration: 280000\n",
      "Coef: [[ 1.83736516]\n",
      " [30.02734924]]\n",
      "Loss: 16568.600817051294\n",
      "Iteration: 290000\n",
      "Coef: [[ 1.91603248]\n",
      " [30.0271076 ]]\n",
      "Loss: 16425.233049067523\n",
      "Iteration: 300000\n",
      "Coef: [[ 1.99438914]\n",
      " [30.02698439]]\n",
      "Loss: 16283.238951881598\n",
      "Iteration: 310000\n",
      "Coef: [[ 2.07238051]\n",
      " [30.02700826]]\n",
      "Loss: 16143.199036717479\n",
      "Iteration: 320000\n",
      "Coef: [[ 2.15002764]\n",
      " [30.02675567]]\n",
      "Loss: 16003.128329092015\n",
      "Iteration: 330000\n",
      "Coef: [[ 2.22734453]\n",
      " [30.02676935]]\n",
      "Loss: 15864.840234676734\n",
      "Iteration: 340000\n",
      "Coef: [[ 2.30432034]\n",
      " [30.02652938]]\n",
      "Loss: 15727.423704396804\n",
      "Iteration: 350000\n",
      "Coef: [[ 2.380995  ]\n",
      " [30.02640735]]\n",
      "Loss: 15591.478221923186\n",
      "Iteration: 360000\n",
      "Coef: [[ 2.45731196]\n",
      " [30.02642932]]\n",
      "Loss: 15457.431089939888\n",
      "Iteration: 370000\n",
      "Coef: [[ 2.53329248]\n",
      " [30.02617933]]\n",
      "Loss: 15323.315735616849\n",
      "Iteration: 380000\n",
      "Coef: [[ 2.60894981]\n",
      " [30.02619846]]\n",
      "Loss: 15190.930583540205\n",
      "Iteration: 390000\n",
      "Coef: [[ 2.68427325]\n",
      " [30.02595498]]\n",
      "Loss: 15059.335410683605\n",
      "Iteration: 400000\n",
      "Coef: [[ 2.75929882]\n",
      " [30.02584384]]\n",
      "Loss: 14929.099237797229\n",
      "Iteration: 410000\n",
      "Coef: [[ 2.83397771]\n",
      " [30.02585897]]\n",
      "Loss: 14800.695125378623\n",
      "Iteration: 420000\n",
      "Coef: [[ 2.90832716]\n",
      " [30.02561726]]\n",
      "Loss: 14672.352319744447\n",
      "Iteration: 430000\n",
      "Coef: [[ 2.98235742]\n",
      " [30.02562556]]\n",
      "Loss: 14545.445671076062\n",
      "Iteration: 440000\n",
      "Coef: [[ 3.05609132]\n",
      " [30.02551065]]\n",
      "Loss: 14419.53485273788\n",
      "Iteration: 450000\n",
      "Coef: [[ 3.12948222]\n",
      " [30.02528445]]\n",
      "Loss: 14294.916210991696\n",
      "Iteration: 460000\n",
      "Coef: [[ 3.20255493]\n",
      " [30.02529937]]\n",
      "Loss: 14171.829392243935\n",
      "Iteration: 470000\n",
      "Coef: [[ 3.27530844]\n",
      " [30.0250692 ]]\n",
      "Loss: 14049.013542303164\n",
      "Iteration: 480000\n",
      "Coef: [[ 3.34774972]\n",
      " [30.02507448]]\n",
      "Loss: 13927.517910701285\n",
      "Iteration: 490000\n",
      "Coef: [[ 3.41990094]\n",
      " [30.02496332]]\n",
      "Loss: 13806.970159947381\n",
      "Iteration: 500000\n",
      "Coef: [[ 3.49171636]\n",
      " [30.02473886]]\n",
      "Loss: 13687.663134077862\n",
      "Iteration: 510000\n",
      "Coef: [[ 3.56322079]\n",
      " [30.02475965]]\n",
      "Loss: 13569.864211442333\n",
      "Iteration: 520000\n",
      "Coef: [[ 3.63441289]\n",
      " [30.02452766]]\n",
      "Loss: 13452.234413121318\n",
      "Iteration: 530000\n",
      "Coef: [[ 3.70529909]\n",
      " [30.02454186]]\n",
      "Loss: 13335.923642341686\n",
      "Iteration: 540000\n",
      "Coef: [[ 3.77587535]\n",
      " [30.02431815]]\n",
      "Loss: 13220.474266989579\n",
      "Iteration: 550000\n",
      "Coef: [[ 3.84617292]\n",
      " [30.02421157]]\n",
      "Loss: 13106.156137560469\n",
      "Iteration: 560000\n",
      "Coef: [[ 3.91614247]\n",
      " [30.02422553]]\n",
      "Loss: 12993.331084855165\n",
      "Iteration: 570000\n",
      "Coef: [[ 3.98580611]\n",
      " [30.02400018]]\n",
      "Loss: 12880.770272416286\n",
      "Iteration: 580000\n",
      "Coef: [[ 4.05517088]\n",
      " [30.02401668]]\n",
      "Loss: 12769.410439160947\n",
      "Iteration: 590000\n",
      "Coef: [[ 4.12425531]\n",
      " [30.0239009 ]]\n",
      "Loss: 12658.781234571645\n",
      "Iteration: 600000\n",
      "Coef: [[ 4.19302068]\n",
      " [30.0236865 ]]\n",
      "Loss: 12549.418915820668\n",
      "Iteration: 610000\n",
      "Coef: [[ 4.26148578]\n",
      " [30.02370001]]\n",
      "Loss: 12441.23519481375\n",
      "Iteration: 620000\n",
      "Coef: [[ 4.32965403]\n",
      " [30.02348801]]\n",
      "Loss: 12333.521794389384\n",
      "Iteration: 630000\n",
      "Coef: [[ 4.39752962]\n",
      " [30.02350043]]\n",
      "Loss: 12226.933188141855\n",
      "Iteration: 640000\n",
      "Coef: [[ 4.46513083]\n",
      " [30.02338991]]\n",
      "Loss: 12121.027043840046\n",
      "Iteration: 650000\n",
      "Coef: [[ 4.53241724]\n",
      " [30.02318368]]\n",
      "Loss: 12016.257022707621\n",
      "Iteration: 660000\n",
      "Coef: [[ 4.59941264]\n",
      " [30.02318739]]\n",
      "Loss: 11912.638106952687\n",
      "Iteration: 670000\n",
      "Coef: [[ 4.6661176 ]\n",
      " [30.02298558]]\n",
      "Loss: 11809.545239417666\n",
      "Iteration: 680000\n",
      "Coef: [[ 4.73253617]\n",
      " [30.02299559]]\n",
      "Loss: 11707.50768605348\n",
      "Iteration: 690000\n",
      "Coef: [[ 4.79868656]\n",
      " [30.02288762]]\n",
      "Loss: 11606.103207382797\n",
      "Iteration: 700000\n",
      "Coef: [[ 4.864529 ]\n",
      " [30.0226861]]\n",
      "Loss: 11505.781915908425\n",
      "Iteration: 710000\n",
      "Coef: [[ 4.93008659]\n",
      " [30.02268948]]\n",
      "Loss: 11406.563769028638\n",
      "Iteration: 720000\n",
      "Coef: [[ 4.99535947]\n",
      " [30.02248969]]\n",
      "Loss: 11307.884127556872\n",
      "Iteration: 730000\n",
      "Coef: [[ 5.06035241]\n",
      " [30.02250608]]\n",
      "Loss: 11210.2035280789\n",
      "Iteration: 740000\n",
      "Coef: [[ 5.12505839]\n",
      " [30.0222945 ]]\n",
      "Loss: 11113.104820227189\n",
      "Iteration: 750000\n",
      "Coef: [[ 5.1895089 ]\n",
      " [30.02220026]]\n",
      "Loss: 11016.987284316101\n",
      "Iteration: 760000\n",
      "Coef: [[ 5.253659  ]\n",
      " [30.02220376]]\n",
      "Loss: 10922.01750979285\n",
      "Iteration: 770000\n",
      "Coef: [[ 5.31753112]\n",
      " [30.02200552]]\n",
      "Loss: 10827.52117716506\n",
      "Iteration: 780000\n",
      "Coef: [[ 5.38112645]\n",
      " [30.02201468]]\n",
      "Loss: 10733.873206434448\n",
      "Iteration: 790000\n",
      "Coef: [[ 5.44446701]\n",
      " [30.02191625]]\n",
      "Loss: 10640.956755850018\n",
      "Iteration: 800000\n",
      "Coef: [[ 5.50751324]\n",
      " [30.02171908]]\n",
      "Loss: 10548.992653695492\n",
      "Iteration: 810000\n",
      "Coef: [[ 5.57028386]\n",
      " [30.02172328]]\n",
      "Loss: 10457.965787635258\n",
      "Iteration: 820000\n",
      "Coef: [[ 5.63278476]\n",
      " [30.02153462]]\n",
      "Loss: 10367.525474312639\n",
      "Iteration: 830000\n",
      "Coef: [[ 5.69501518]\n",
      " [30.02154087]]\n",
      "Loss: 10277.86456471846\n",
      "Iteration: 840000\n",
      "Coef: [[ 5.75699618]\n",
      " [30.02144562]]\n",
      "Loss: 10188.907508358647\n",
      "Iteration: 850000\n",
      "Coef: [[ 5.81868886]\n",
      " [30.02125019]]\n",
      "Loss: 10100.86809387809\n",
      "Iteration: 860000\n",
      "Coef: [[ 5.88011207]\n",
      " [30.02126053]]\n",
      "Loss: 10013.750733944411\n",
      "Iteration: 870000\n",
      "Coef: [[ 5.94126881]\n",
      " [30.02107567]]\n",
      "Loss: 9927.050394744869\n",
      "Iteration: 880000\n",
      "Coef: [[ 6.00216326]\n",
      " [30.02107689]]\n",
      "Loss: 9841.22642528398\n",
      "Iteration: 890000\n",
      "Coef: [[ 6.06281392]\n",
      " [30.02098447]]\n",
      "Loss: 9756.059578441347\n",
      "Iteration: 900000\n",
      "Coef: [[ 6.12318219]\n",
      " [30.02079238]]\n",
      "Loss: 9671.775167880805\n",
      "Iteration: 910000\n",
      "Coef: [[ 6.18328692]\n",
      " [30.02080694]]\n",
      "Loss: 9588.397281197333\n",
      "Iteration: 920000\n",
      "Coef: [[ 6.24313113]\n",
      " [30.02062028]]\n",
      "Loss: 9505.364398980806\n",
      "Iteration: 930000\n",
      "Coef: [[ 6.30271831]\n",
      " [30.02062975]]\n",
      "Loss: 9423.209102911647\n",
      "Iteration: 940000\n",
      "Coef: [[ 6.36206466]\n",
      " [30.02053146]]\n",
      "Loss: 9341.590444076724\n",
      "Iteration: 950000\n",
      "Coef: [[ 6.42113709]\n",
      " [30.0203472 ]]\n",
      "Loss: 9260.891891864208\n",
      "Iteration: 960000\n",
      "Coef: [[ 6.47995164]\n",
      " [30.02036041]]\n",
      "Loss: 9181.067632302185\n",
      "Iteration: 970000\n",
      "Coef: [[ 6.53851114]\n",
      " [30.02017538]]\n",
      "Loss: 9101.585665189607\n",
      "Iteration: 980000\n",
      "Coef: [[ 6.59681938]\n",
      " [30.02019017]]\n",
      "Loss: 9022.930873397401\n",
      "Iteration: 990000\n",
      "Coef: [[ 6.65487013]\n",
      " [30.02000472]]\n",
      "Loss: 8944.772297118348\n",
      "Iteration: 1000000\n",
      "Coef: [[ 6.71269381]\n",
      " [30.01991332]]\n",
      "Loss: 8867.453755779381\n",
      "Iteration: 1010000\n",
      "Coef: [[ 6.77024585]\n",
      " [30.01992184]]\n",
      "Loss: 8791.01517149948\n",
      "Iteration: 1020000\n",
      "Coef: [[ 6.82754854]\n",
      " [30.01974312]]\n",
      "Loss: 8714.92109463296\n",
      "Iteration: 1030000\n",
      "Coef: [[ 6.88460282]\n",
      " [30.01974788]]\n",
      "Loss: 8639.528827727963\n",
      "Iteration: 1040000\n",
      "Coef: [[ 6.94140979]\n",
      " [30.01957343]]\n",
      "Loss: 8564.79316278996\n",
      "Iteration: 1050000\n",
      "Coef: [[ 6.99799233]\n",
      " [30.0194829 ]]\n",
      "Loss: 8490.764831106924\n",
      "Iteration: 1060000\n",
      "Coef: [[ 7.05430674]\n",
      " [30.0194895 ]]\n",
      "Loss: 8417.485822547063\n",
      "Iteration: 1070000\n",
      "Coef: [[ 7.1103792 ]\n",
      " [30.01932145]]\n",
      "Loss: 8344.66889473865\n",
      "Iteration: 1080000\n",
      "Coef: [[ 7.16621092]\n",
      " [30.01933257]]\n",
      "Loss: 8272.59352732843\n",
      "Iteration: 1090000\n",
      "Coef: [[ 7.22179632]\n",
      " [30.0191527 ]]\n",
      "Loss: 8200.948058518092\n",
      "Iteration: 1100000\n",
      "Coef: [[ 7.27716408]\n",
      " [30.01906159]]\n",
      "Loss: 8130.08130296293\n",
      "Iteration: 1110000\n",
      "Coef: [[ 7.33226987]\n",
      " [30.01907393]]\n",
      "Loss: 8059.937371713513\n",
      "Iteration: 1120000\n",
      "Coef: [[ 7.38713867]\n",
      " [30.01890227]]\n",
      "Loss: 7990.222134828768\n",
      "Iteration: 1130000\n",
      "Coef: [[ 7.44176992]\n",
      " [30.01891339]]\n",
      "Loss: 7921.115680324761\n",
      "Iteration: 1140000\n",
      "Coef: [[ 7.49616193]\n",
      " [30.01874258]]\n",
      "Loss: 7852.549683217399\n",
      "Iteration: 1150000\n",
      "Coef: [[ 7.55034122]\n",
      " [30.01865247]]\n",
      "Loss: 7784.699841375303\n",
      "Iteration: 1160000\n",
      "Coef: [[ 7.60426201]\n",
      " [30.0186577 ]]\n",
      "Loss: 7717.445744053733\n",
      "Iteration: 1170000\n",
      "Coef: [[ 7.65795281]\n",
      " [30.01850125]]\n",
      "Loss: 7650.728624640248\n",
      "Iteration: 1180000\n",
      "Coef: [[ 7.71141125]\n",
      " [30.01850408]]\n",
      "Loss: 7584.582976568321\n",
      "Iteration: 1190000\n",
      "Coef: [[ 7.7646557 ]\n",
      " [30.01842202]]\n",
      "Loss: 7518.939833962651\n",
      "Iteration: 1200000\n",
      "Coef: [[ 7.81765216]\n",
      " [30.01825348]]\n",
      "Loss: 7453.983115242958\n",
      "Iteration: 1210000\n",
      "Coef: [[ 7.87041538]\n",
      " [30.01825762]]\n",
      "Loss: 7389.601741752524\n",
      "Iteration: 1220000\n",
      "Coef: [[ 7.92295372]\n",
      " [30.01810245]]\n",
      "Loss: 7325.7287431991335\n",
      "Iteration: 1230000\n",
      "Coef: [[ 7.97526484]\n",
      " [30.01810885]]\n",
      "Loss: 7262.394837012767\n",
      "Iteration: 1240000\n",
      "Coef: [[ 8.02736636]\n",
      " [30.01802965]]\n",
      "Loss: 7199.538799783822\n",
      "Iteration: 1250000\n",
      "Coef: [[ 8.07922342]\n",
      " [30.01786668]]\n",
      "Loss: 7137.295697238309\n",
      "Iteration: 1260000\n",
      "Coef: [[ 8.13085622]\n",
      " [30.01786897]]\n",
      "Loss: 7075.719271107373\n",
      "Iteration: 1270000\n",
      "Coef: [[ 8.18226692]\n",
      " [30.01770933]]\n",
      "Loss: 7014.566643317277\n",
      "Iteration: 1280000\n",
      "Coef: [[ 8.23345321]\n",
      " [30.01771708]]\n",
      "Loss: 6953.848076344354\n",
      "Iteration: 1290000\n",
      "Coef: [[ 8.28443624]\n",
      " [30.01764127]]\n",
      "Loss: 6893.681822262175\n",
      "Iteration: 1300000\n",
      "Coef: [[ 8.33518004]\n",
      " [30.0174825 ]]\n",
      "Loss: 6834.092650276253\n",
      "Iteration: 1310000\n",
      "Coef: [[ 8.38570247]\n",
      " [30.01748018]]\n",
      "Loss: 6775.068014449364\n",
      "Iteration: 1320000\n",
      "Coef: [[ 8.43600959]\n",
      " [30.01733366]]\n",
      "Loss: 6716.524538282456\n",
      "Iteration: 1330000\n",
      "Coef: [[ 8.48609674]\n",
      " [30.01733525]]\n",
      "Loss: 6658.424949809613\n",
      "Iteration: 1340000\n",
      "Coef: [[ 8.53598542]\n",
      " [30.01726197]]\n",
      "Loss: 6600.823573341667\n",
      "Iteration: 1350000\n",
      "Coef: [[ 8.58564024]\n",
      " [30.01710769]]\n",
      "Loss: 6543.765037732413\n",
      "Iteration: 1360000\n",
      "Coef: [[ 8.63507804]\n",
      " [30.01710588]]\n",
      "Loss: 6487.261909090622\n",
      "Iteration: 1370000\n",
      "Coef: [[ 8.68430533]\n",
      " [30.01695934]]\n",
      "Loss: 6431.216213864783\n",
      "Iteration: 1380000\n",
      "Coef: [[ 8.73331757]\n",
      " [30.01696524]]\n",
      "Loss: 6375.579537018346\n",
      "Iteration: 1390000\n",
      "Coef: [[ 8.78211732]\n",
      " [30.01681311]]\n",
      "Loss: 6320.428362702312\n",
      "Iteration: 1400000\n",
      "Coef: [[ 8.83072399]\n",
      " [30.016734  ]]\n",
      "Loss: 6265.80806872161\n",
      "Iteration: 1410000\n",
      "Coef: [[ 8.87910064]\n",
      " [30.01674475]]\n",
      "Loss: 6211.744609454393\n",
      "Iteration: 1420000\n",
      "Coef: [[ 8.92726917]\n",
      " [30.0165941 ]]\n",
      "Loss: 6158.018131376039\n",
      "Iteration: 1430000\n",
      "Coef: [[ 8.97522942]\n",
      " [30.01660288]]\n",
      "Loss: 6104.748880535659\n",
      "Iteration: 1440000\n",
      "Coef: [[ 9.02299752]\n",
      " [30.01652686]]\n",
      "Loss: 6051.897326235958\n",
      "Iteration: 1450000\n",
      "Coef: [[ 9.07054345]\n",
      " [30.0163803 ]]\n",
      "Loss: 5999.595196400654\n",
      "Iteration: 1460000\n",
      "Coef: [[ 9.11788166]\n",
      " [30.01638141]]\n",
      "Loss: 5947.811462242253\n",
      "Iteration: 1470000\n",
      "Coef: [[ 9.16501801]\n",
      " [30.01623248]]\n",
      "Loss: 5896.4727387667035\n",
      "Iteration: 1480000\n",
      "Coef: [[ 9.21194722]\n",
      " [30.01624346]]\n",
      "Loss: 5845.383552541487\n",
      "Iteration: 1490000\n",
      "Coef: [[ 9.25867436]\n",
      " [30.01609735]]\n",
      "Loss: 5794.835962398217\n",
      "Iteration: 1500000\n",
      "Coef: [[ 9.30521502]\n",
      " [30.01602807]]\n",
      "Loss: 5744.7234651440685\n",
      "Iteration: 1510000\n",
      "Coef: [[ 9.35153726]\n",
      " [30.01603039]]\n",
      "Loss: 5695.144300737103\n",
      "Iteration: 1520000\n",
      "Coef: [[ 9.3976602 ]\n",
      " [30.01589076]]\n",
      "Loss: 5645.905288119973\n",
      "Iteration: 1530000\n",
      "Coef: [[ 9.44358339]\n",
      " [30.01589903]]\n",
      "Loss: 5597.094368029845\n",
      "Iteration: 1540000\n",
      "Coef: [[ 9.48932267]\n",
      " [30.01582855]]\n",
      "Loss: 5548.644335640392\n",
      "Iteration: 1550000\n",
      "Coef: [[ 9.53484898]\n",
      " [30.01567624]]\n",
      "Loss: 5500.717439449032\n",
      "Iteration: 1560000\n",
      "Coef: [[ 9.58017511]\n",
      " [30.0156869 ]]\n",
      "Loss: 5453.211777959822\n",
      "Iteration: 1570000\n",
      "Coef: [[ 9.62530814]\n",
      " [30.0155484 ]]\n",
      "Loss: 5406.067532692823\n",
      "Iteration: 1580000\n",
      "Coef: [[ 9.67024393]\n",
      " [30.01555206]]\n",
      "Loss: 5359.27675413098\n",
      "Iteration: 1590000\n",
      "Coef: [[ 9.71498661]\n",
      " [30.01541307]]\n",
      "Loss: 5312.944507851411\n",
      "Iteration: 1600000\n",
      "Coef: [[ 9.75955054]\n",
      " [30.01534405]]\n",
      "Loss: 5267.012530693331\n",
      "Iteration: 1610000\n",
      "Coef: [[ 9.80390355]\n",
      " [30.01534834]]\n",
      "Loss: 5221.529857206026\n",
      "Iteration: 1620000\n",
      "Coef: [[ 9.8480676 ]\n",
      " [30.01521462]]\n",
      "Loss: 5176.406796674646\n",
      "Iteration: 1630000\n",
      "Coef: [[ 9.89203891]\n",
      " [30.01521867]]\n",
      "Loss: 5131.608019525473\n",
      "Iteration: 1640000\n",
      "Coef: [[ 9.9358356 ]\n",
      " [30.01515332]]\n",
      "Loss: 5087.2114244568265\n",
      "Iteration: 1650000\n",
      "Coef: [[ 9.97942858]\n",
      " [30.01501215]]\n",
      "Loss: 5043.272228989353\n",
      "Iteration: 1660000\n",
      "Coef: [[10.0228296 ]\n",
      " [30.01502139]]\n",
      "Loss: 4999.736634689413\n",
      "Iteration: 1670000\n",
      "Coef: [[10.06604556]\n",
      " [30.0148834 ]]\n",
      "Loss: 4956.547998106991\n",
      "Iteration: 1680000\n",
      "Coef: [[10.10907139]\n",
      " [30.01489006]]\n",
      "Loss: 4913.595109973069\n",
      "Iteration: 1690000\n",
      "Coef: [[10.15192778]\n",
      " [30.01482778]]\n",
      "Loss: 4871.096395339494\n",
      "Iteration: 1700000\n",
      "Coef: [[10.19458332]\n",
      " [30.01469604]]\n",
      "Loss: 4828.9951450067065\n",
      "Iteration: 1710000\n",
      "Coef: [[10.23705269]\n",
      " [30.01469475]]\n",
      "Loss: 4787.296368909705\n",
      "Iteration: 1720000\n",
      "Coef: [[10.27934109]\n",
      " [30.01456815]]\n",
      "Loss: 4745.9419407302485\n",
      "Iteration: 1730000\n",
      "Coef: [[10.32144491]\n",
      " [30.01457427]]\n",
      "Loss: 4704.881928115956\n",
      "Iteration: 1740000\n",
      "Coef: [[10.36336607]\n",
      " [30.01444243]]\n",
      "Loss: 4664.18419861641\n",
      "Iteration: 1750000\n",
      "Coef: [[10.40512132]\n",
      " [30.01437437]]\n",
      "Loss: 4623.878656458757\n",
      "Iteration: 1760000\n",
      "Coef: [[10.44667724]\n",
      " [30.01438005]]\n",
      "Loss: 4583.9307741031835\n",
      "Iteration: 1770000\n",
      "Coef: [[10.48805782]\n",
      " [30.01425299]]\n",
      "Loss: 4544.347466761148\n",
      "Iteration: 1780000\n",
      "Coef: [[10.52925623]\n",
      " [30.01425822]]\n",
      "Loss: 4504.9897404668645\n",
      "Iteration: 1790000\n",
      "Coef: [[10.57029248]\n",
      " [30.01420055]]\n",
      "Loss: 4466.032422004691\n",
      "Iteration: 1800000\n",
      "Coef: [[10.61113635]\n",
      " [30.01406648]]\n",
      "Loss: 4427.4446829691105\n",
      "Iteration: 1810000\n",
      "Coef: [[10.65180054]\n",
      " [30.01407041]]\n",
      "Loss: 4389.188678192252\n",
      "Iteration: 1820000\n",
      "Coef: [[10.69229272]\n",
      " [30.01394628]]\n",
      "Loss: 4351.301505843259\n",
      "Iteration: 1830000\n",
      "Coef: [[10.73260671]\n",
      " [30.01395392]]\n",
      "Loss: 4313.621088109258\n",
      "Iteration: 1840000\n",
      "Coef: [[10.77274741]\n",
      " [30.01382791]]\n",
      "Loss: 4276.320793288761\n",
      "Iteration: 1850000\n",
      "Coef: [[10.81272762]\n",
      " [30.01376717]]\n",
      "Loss: 4239.3467582482635\n",
      "Iteration: 1860000\n",
      "Coef: [[10.85252031]\n",
      " [30.01377343]]\n",
      "Loss: 4202.77802302867\n",
      "Iteration: 1870000\n",
      "Coef: [[10.89214204]\n",
      " [30.01364793]]\n",
      "Loss: 4166.440079239763\n",
      "Iteration: 1880000\n",
      "Coef: [[10.93159057]\n",
      " [30.01365469]]\n",
      "Loss: 4130.373147502021\n",
      "Iteration: 1890000\n",
      "Coef: [[10.97088258]\n",
      " [30.01359447]]\n",
      "Loss: 4094.6309728032884\n",
      "Iteration: 1900000\n",
      "Coef: [[11.00999186]\n",
      " [30.01347029]]\n",
      "Loss: 4059.257006972629\n",
      "Iteration: 1910000\n",
      "Coef: [[11.04892899]\n",
      " [30.01347336]]\n",
      "Loss: 4024.194164931454\n",
      "Iteration: 1920000\n",
      "Coef: [[11.08770149]\n",
      " [30.01335094]]\n",
      "Loss: 3989.478742284453\n",
      "Iteration: 1930000\n",
      "Coef: [[11.12630188]\n",
      " [30.01335912]]\n",
      "Loss: 3954.892823520838\n",
      "Iteration: 1940000\n",
      "Coef: [[11.16475202]\n",
      " [30.01330696]]\n",
      "Loss: 3920.700896393305\n",
      "Iteration: 1950000\n",
      "Coef: [[11.20302021]\n",
      " [30.01318166]]\n",
      "Loss: 3886.80961239035\n",
      "Iteration: 1960000\n",
      "Coef: [[11.2411216 ]\n",
      " [30.01318411]]\n",
      "Loss: 3853.2388900346295\n",
      "Iteration: 1970000\n",
      "Coef: [[11.27906176]\n",
      " [30.01306273]]\n",
      "Loss: 3820.01315985406\n",
      "Iteration: 1980000\n",
      "Coef: [[11.31683215]\n",
      " [30.01306875]]\n",
      "Loss: 3786.863124606589\n",
      "Iteration: 1990000\n",
      "Coef: [[11.35445685]\n",
      " [30.01301825]]\n",
      "Loss: 3754.130165021059\n",
      "Iteration: 2000000\n",
      "Coef: [[11.39190379]\n",
      " [30.01290157]]\n",
      "Loss: 3721.678650254275\n",
      "Iteration: 2010000\n",
      "Coef: [[11.42918867]\n",
      " [30.01290508]]\n",
      "Loss: 3689.582718722461\n",
      "Iteration: 2020000\n",
      "Coef: [[11.46631304]\n",
      " [30.01278341]]\n",
      "Loss: 3657.7200225937363\n",
      "Iteration: 2030000\n",
      "Coef: [[11.5032727 ]\n",
      " [30.01278797]]\n",
      "Loss: 3625.98878471203\n",
      "Iteration: 2040000\n",
      "Coef: [[11.54008963]\n",
      " [30.01273995]]\n",
      "Loss: 3594.652230112668\n",
      "Iteration: 2050000\n",
      "Coef: [[11.57673255]\n",
      " [30.01262143]]\n",
      "Loss: 3563.587182292388\n",
      "Iteration: 2060000\n",
      "Coef: [[11.61321558]\n",
      " [30.01262652]]\n",
      "Loss: 3532.8286744104207\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m Linear_Regression(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m, init_weight\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmat([\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m25\u001b[39m])\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[236], line 61\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m),)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[236], line 124\u001b[0m, in \u001b[0;36mLinear_Regression.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iter):\n\u001b[1;32m    122\u001b[0m     \n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef\n\u001b[1;32m    125\u001b[0m     errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m-\u001b[39m preds\n\u001b[1;32m    126\u001b[0m     squared_errors \u001b[38;5;241m=\u001b[39m (errors\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30]. \n",
    "#### It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.55341784]\n",
      " [ 0.04632668]]\n",
      "Loss: 1.298640205014184e+16\n",
      "EARLY STOP @ 1308\n"
     ]
    }
   ],
   "source": [
    "X_norm = min_max_normalize(X)\n",
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X_norm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   50.],\n",
       "       [29850.]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 996, 995)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(), X.max(), X.max() - X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 29850 / 995 = 30 \n",
    "# slope checks out\n",
    "\n",
    "# x - min. every single point is shifted over by the minimum value. line is shifted over by the slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine[['density','alcohol']]\n",
    "X_norm = min_max_normalize(X)\n",
    "y = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800.6676988774323"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "## Squared Error with sklearn.\n",
    "sum((lr.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.55341784]\n",
      " [ 0.04632668]\n",
      " [ 0.10140291]]\n",
      "Loss: 5287551503614576.0\n",
      "Iteration: 10000\n",
      "Coef: [[0.22566935]\n",
      " [0.83438408]\n",
      " [0.43902616]]\n",
      "Loss: 816.584217720002\n",
      "Iteration: 20000\n",
      "Coef: [[0.48625587]\n",
      " [1.10121317]\n",
      " [0.38829796]]\n",
      "Loss: 806.863747729821\n",
      "Iteration: 30000\n",
      "Coef: [[0.57608994]\n",
      " [1.19622891]\n",
      " [0.3710301 ]]\n",
      "Loss: 805.6710149577236\n",
      "Iteration: 40000\n",
      "Coef: [[0.60605507]\n",
      " [1.23100601]\n",
      " [0.36485609]]\n",
      "Loss: 805.524723167701\n",
      "Iteration: 50000\n",
      "Coef: [[0.61504279]\n",
      " [1.24467339]\n",
      " [0.36267951]]\n",
      "Loss: 805.5062096751012\n",
      "Iteration: 60000\n",
      "Coef: [[0.61669178]\n",
      " [1.25095649]\n",
      " [0.3619387 ]]\n",
      "Loss: 805.5033619800577\n",
      "Iteration: 70000\n",
      "Coef: [[0.6157666 ]\n",
      " [1.25464965]\n",
      " [0.36167055]]\n",
      "Loss: 805.5024306852986\n",
      "Iteration: 80000\n",
      "Coef: [[0.6139414 ]\n",
      " [1.25743687]\n",
      " [0.36158585]]\n",
      "Loss: 805.5017347855267\n",
      "Iteration: 90000\n",
      "Coef: [[0.61179927]\n",
      " [1.25990672]\n",
      " [0.36155593]]\n",
      "Loss: 805.5010687469071\n",
      "Iteration: 100000\n",
      "Coef: [[0.60954709]\n",
      " [1.26226356]\n",
      " [0.36154123]]\n",
      "Loss: 805.5004057997098\n",
      "Iteration: 110000\n",
      "Coef: [[0.60725718]\n",
      " [1.26458244]\n",
      " [0.36154434]]\n",
      "Loss: 805.4997461171847\n",
      "Iteration: 120000\n",
      "Coef: [[0.60495283]\n",
      " [1.26688648]\n",
      " [0.36153918]]\n",
      "Loss: 805.4990846639216\n",
      "Iteration: 130000\n",
      "Coef: [[0.60264512]\n",
      " [1.26918649]\n",
      " [0.36154613]]\n",
      "Loss: 805.4984212859487\n",
      "Iteration: 140000\n",
      "Coef: [[0.60033442]\n",
      " [1.27148533]\n",
      " [0.36154906]]\n",
      "Loss: 805.4977605115037\n",
      "Iteration: 150000\n",
      "Coef: [[0.59802382]\n",
      " [1.27378166]\n",
      " [0.3615456 ]]\n",
      "Loss: 805.4970980785982\n",
      "Iteration: 160000\n",
      "Coef: [[0.59571387]\n",
      " [1.27607884]\n",
      " [0.3615522 ]]\n",
      "Loss: 805.496438494674\n",
      "Iteration: 170000\n",
      "Coef: [[0.59340307]\n",
      " [1.27837481]\n",
      " [0.36154877]]\n",
      "Loss: 805.4957778553885\n",
      "Iteration: 180000\n",
      "Coef: [[0.59109358]\n",
      " [1.28067147]\n",
      " [0.36155611]]\n",
      "Loss: 805.4951151495604\n",
      "Iteration: 190000\n",
      "Coef: [[0.5887828 ]\n",
      " [1.2829685 ]\n",
      " [0.36155868]]\n",
      "Loss: 805.4944542719361\n",
      "Iteration: 200000\n",
      "Coef: [[0.58647262]\n",
      " [1.28526386]\n",
      " [0.36155561]]\n",
      "Loss: 805.4937929941775\n",
      "Iteration: 210000\n",
      "Coef: [[0.58416339]\n",
      " [1.2875602 ]\n",
      " [0.36156275]]\n",
      "Loss: 805.493134494281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[265], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr2 \u001b[38;5;241m=\u001b[39m Linear_Regression(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m, init_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlr2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[251], line 61\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m),)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[251], line 159\u001b[0m, in \u001b[0;36mLinear_Regression.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m temp_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m dcoef\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# print(self.coef.shape, temp_coef.shape, (self.alpha * dcoef).shape)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(self.coef, temp_coef.shape)\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m temp_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_coef\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# technically double calculates, which doesn't need to be done. i'll fix this later.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m pre_error \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    163\u001b[0m current_error \u001b[38;5;241m=\u001b[39m temp_loss\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/dsc80_39/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/dsc80_39/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr2 = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight=None)\n",
    "lr2.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.42268462]\n",
      " [ 0.11000215]\n",
      " [ 0.14338641]]\n",
      "Loss: 57822.871677132076\n",
      "EARLY STOP @ 3624\n"
     ]
    }
   ],
   "source": [
    "lr2.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just fucking instantaneous once again\n",
    "# that's actually kinda insane to me\n",
    "# i'm not sure why...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 5000000)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((clf.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[1. 1.]\n",
      " [1. 2.]\n",
      " [1. 3.]]\n",
      "[[-0.55341784]\n",
      " [ 0.04632668]]\n",
      "Iteration: 0\n",
      "Coef: [[-0.53865325]\n",
      " [ 0.07967055]]\n",
      "Loss: 18.677391590518212\n",
      "EARLY STOP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.33083181e-15],\n",
       "       [ 1.00000000e+00]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tests \n",
    "\n",
    "test = Linear_Regression(intercept=True, verbose=True, alpha=1e-3)\n",
    "X2 = np.array([[1], [2], [3]])\n",
    "y2 = np.array([1, 2, 3])\n",
    "X2, y2\n",
    "test.fit(X2, y2)\n",
    "test.coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
