{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None, verbose = False):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "            \n",
    "            attributes: \n",
    "                        alpha: Learning Rate, default 1e-10\n",
    "                        num_iter: Number of Iterations to update coefficient with training data\n",
    "                        early_stop: Constant control early_stop.\n",
    "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
    "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                        \n",
    "            \n",
    "            TODO: 1. Initialize all variables needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Linear Regression'\n",
    "\n",
    "        # store the variables in object scope\n",
    "        self.alpha: float = alpha\n",
    "        self.num_iter: int = num_iter\n",
    "        self.early_stop: float = early_stop\n",
    "        self.intercept: bool = intercept\n",
    "        self.init_weight = init_weight  ### For testing correctness.\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and perform gradient descent.\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "                \n",
    "                \n",
    "                TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
    "                      3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
    "                      4. Call the gradient_descent function to train.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = np.array(X_train)\n",
    "        self.y = np.array(y_train).reshape(-1, 1) \n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones((self.X.shape[0], 1))\n",
    "            self.X = np.hstack([ones, self.X])\n",
    "            self.log(self.X.shape)\n",
    "            self.log(self.X)\n",
    "        \n",
    "        if self.init_weight is None:\n",
    "            np.random.seed(16)\n",
    "            self.coef = np.random.uniform(-1, 1, (self.X.shape[1], 1),)\n",
    "            self.log(self.coef)\n",
    "        else:\n",
    "            self.coef = np.array(self.init_weight).reshape(self.X.shape[1], 1)\n",
    "        \n",
    "        self.gradient_descent()\n",
    "        \n",
    "        # self.coef = self.init_weight #### Please change this after you get the example right.\n",
    "        \n",
    "    # unused function. functionality merged into gradient_descent\n",
    "    # with a manual backward pass, intermediate forward pass calclations are out of scope\n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "            Helper function to calculate the gradient respect to coefficient.\n",
    "            \n",
    "            TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        d_loss = 1\n",
    "        d_diff = d_loss * 2 * (self.y - self.X @ self.coef)\n",
    "        d_pred = d_diff * -1\n",
    "        d_coef = (np.asarray(d_pred) * np.asarray(self.X)).sum(axis=0)\n",
    "        \n",
    "        # doesn't make sense to have to calculate these values twice\n",
    "        # i can just keep the gradient inside the gradient_descent function\n",
    "        grad_coef = (-2 * (self.y - self.X @ self.coef) * self.X).sum(axis=0)\n",
    "        \n",
    "        print(np.isclose(d_coef, grad_coef))\n",
    "        \n",
    "        self.grad_coef = grad_coef\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Training function\n",
    "            \n",
    "            TODO: 6. Calculate the loss with current coefficients.\n",
    "                  7. Update the temp_coef with learning rate and gradient.\n",
    "                  8. Calculate the loss with temp_coef.\n",
    "                  9. Implement the self adeptive learning rate. \n",
    "                      a. If current error is less than previous error, increase learning rate by a factor 1.3. \n",
    "                         And update coef, with temp_coef.\n",
    "                      b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
    "                         Don't update coef.\n",
    "                  10. Add the loss to loss list we create.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            # forward pass\n",
    "            preds = self.X @ self.coef\n",
    "            errors = self.y - preds\n",
    "            squared_errors = (errors**2)\n",
    "            loss = squared_errors.sum()\n",
    "            \n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            # backward pass\n",
    "            dloss = 1\n",
    "            dsquared_errors = dloss * np.ones_like(squared_errors)\n",
    "            derrors = dsquared_errors * 2 * errors\n",
    "            dpreds = derrors * -1\n",
    "            # dcoef = (dpreds * self.X).sum(axis=0) # for a given coefficients affects a column of x's, which each affect a different prediction\n",
    "            dcoef = (dpreds * self.X).sum(axis=0).reshape(-1, 1) # for a given coefficients affects a column of x's, which each affect a different prediction\n",
    "\n",
    "            temp_coef = self.coef - self.alpha * dcoef\n",
    "            temp_loss = np.sum((self.y - self.X @ temp_coef)**2)          \n",
    "            pre_error = loss\n",
    "            \n",
    "            current_error = temp_loss\n",
    "            \n",
    "            ### This is the early stop, don't modify fllowing three lines.\n",
    "            if (abs(pre_error - current_error) < self.early_stop) | (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                print(f\"EARLY STOP @ {i}\")\n",
    "                self.coef = temp_coef\n",
    "                return self\n",
    "            \n",
    "            if current_error <= pre_error:\n",
    "                self.alpha *= 1.3\n",
    "                self.coef = temp_coef\n",
    "            else:\n",
    "                self.alpha *= 0.9 # don't update coefs\n",
    "                \n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print('Iteration: ' +  str(i))\n",
    "                print('Coef: '+ str(self.coef))\n",
    "                print('Loss: ' + str(current_error))            \n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the value based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                result: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 11. Implement the prediction function\n",
    "        \"\"\"\n",
    "        # assumes x has intercept added if used here\n",
    "        return np.array(x) @ self.coef\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            X: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                ret: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: 12. Make sure add the 1's column like we did to add intercept.\n",
    "                  13. Revise the following for-loop to call ind_predict to get predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack([ones, X])\n",
    "        \n",
    "        return X @ self.coef\n",
    "        \n",
    "    def log(self, str):\n",
    "        if self.verbose == True:\n",
    "            print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() +  20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[15]\n",
      " [25]]\n",
      "Loss: 2.931949540902693e+25\n",
      "Iteration: 10000\n",
      "Coef: [[15.02874056]\n",
      " [30.00748702]]\n",
      "Loss: 1241.2855503711044\n",
      "Iteration: 20000\n",
      "Coef: [[15.05027193]\n",
      " [30.00742094]]\n",
      "Loss: 1230.5235188302652\n",
      "Iteration: 30000\n",
      "Coef: [[15.07171179]\n",
      " [30.00742077]]\n",
      "Loss: 1219.8884700011508\n",
      "Iteration: 40000\n",
      "Coef: [[15.09305772]\n",
      " [30.00735475]]\n",
      "Loss: 1209.335432801025\n",
      "Iteration: 50000\n",
      "Coef: [[15.11431857]\n",
      " [30.00732366]]\n",
      "Loss: 1198.8753634607592\n",
      "Iteration: 60000\n",
      "Coef: [[15.13548116]\n",
      " [30.00732717]]\n",
      "Loss: 1188.5593446511898\n",
      "Iteration: 70000\n",
      "Coef: [[15.15655031]\n",
      " [30.00726026]]\n",
      "Loss: 1178.2541315732394\n",
      "Iteration: 80000\n",
      "Coef: [[15.17752986]\n",
      " [30.00726407]]\n",
      "Loss: 1168.0756711622355\n",
      "Iteration: 90000\n",
      "Coef: [[15.19841689]\n",
      " [30.00719774]]\n",
      "Loss: 1157.9580560811191\n",
      "Iteration: 100000\n",
      "Coef: [[15.2192214 ]\n",
      " [30.00716703]]\n",
      "Loss: 1147.9435585362207\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m Linear_Regression(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m, init_weight\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmat([\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m25\u001b[39m])\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weight)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 136\u001b[0m, in \u001b[0;36mLinear_Regression.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m    135\u001b[0m dloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 136\u001b[0m dsquared_errors \u001b[38;5;241m=\u001b[39m dloss \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquared_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# dsquared_errors = dloss * 1\u001b[39;00m\n\u001b[1;32m    138\u001b[0m derrors \u001b[38;5;241m=\u001b[39m dsquared_errors \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m errors\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/core/numeric.py:265\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_ones_like_dispatcher)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mones_like\u001b[39m(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    Return an array of ones with the same shape and type as a given array.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     multiarray\u001b[38;5;241m.\u001b[39mcopyto(res, \u001b[38;5;241m1\u001b[39m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/core/multiarray.py:85\u001b[0m, in \u001b[0;36mempty_like\u001b[0;34m(prototype, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# We can't verify dispatcher signatures because NumPy's C functions don't\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# support introspection.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m array_function_from_c_func_and_dispatcher \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     81\u001b[0m     overrides\u001b[38;5;241m.\u001b[39marray_function_from_dispatcher,\n\u001b[1;32m     82\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m, docs_from_dispatcher\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mempty_like)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty_like\u001b[39m(prototype, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    empty_like(prototype, dtype=None, order='K', subok=True, shape=None)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (prototype,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30]. \n",
    "#### It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   45.23173789],\n",
       "       [  195.26753557],\n",
       "       [  345.30333325],\n",
       "       [  495.33913093],\n",
       "       [  645.37492861],\n",
       "       [  795.41072629],\n",
       "       [  945.44652397],\n",
       "       [ 1095.48232165],\n",
       "       [ 1245.51811933],\n",
       "       [ 1395.55391701],\n",
       "       [ 1545.58971468],\n",
       "       [ 1695.62551236],\n",
       "       [ 1845.66131004],\n",
       "       [ 1995.69710772],\n",
       "       [ 2145.7329054 ],\n",
       "       [ 2295.76870308],\n",
       "       [ 2445.80450076],\n",
       "       [ 2595.84029844],\n",
       "       [ 2745.87609612],\n",
       "       [ 2895.9118938 ],\n",
       "       [ 3045.94769147],\n",
       "       [ 3195.98348915],\n",
       "       [ 3346.01928683],\n",
       "       [ 3496.05508451],\n",
       "       [ 3646.09088219],\n",
       "       [ 3796.12667987],\n",
       "       [ 3946.16247755],\n",
       "       [ 4096.19827523],\n",
       "       [ 4246.23407291],\n",
       "       [ 4396.26987059],\n",
       "       [ 4546.30566826],\n",
       "       [ 4696.34146594],\n",
       "       [ 4846.37726362],\n",
       "       [ 4996.4130613 ],\n",
       "       [ 5146.44885898],\n",
       "       [ 5296.48465666],\n",
       "       [ 5446.52045434],\n",
       "       [ 5596.55625202],\n",
       "       [ 5746.5920497 ],\n",
       "       [ 5896.62784738],\n",
       "       [ 6046.66364506],\n",
       "       [ 6196.69944273],\n",
       "       [ 6346.73524041],\n",
       "       [ 6496.77103809],\n",
       "       [ 6646.80683577],\n",
       "       [ 6796.84263345],\n",
       "       [ 6946.87843113],\n",
       "       [ 7096.91422881],\n",
       "       [ 7246.95002649],\n",
       "       [ 7396.98582417],\n",
       "       [ 7547.02162185],\n",
       "       [ 7697.05741952],\n",
       "       [ 7847.0932172 ],\n",
       "       [ 7997.12901488],\n",
       "       [ 8147.16481256],\n",
       "       [ 8297.20061024],\n",
       "       [ 8447.23640792],\n",
       "       [ 8597.2722056 ],\n",
       "       [ 8747.30800328],\n",
       "       [ 8897.34380096],\n",
       "       [ 9047.37959864],\n",
       "       [ 9197.41539632],\n",
       "       [ 9347.45119399],\n",
       "       [ 9497.48699167],\n",
       "       [ 9647.52278935],\n",
       "       [ 9797.55858703],\n",
       "       [ 9947.59438471],\n",
       "       [10097.63018239],\n",
       "       [10247.66598007],\n",
       "       [10397.70177775],\n",
       "       [10547.73757543],\n",
       "       [10697.77337311],\n",
       "       [10847.80917078],\n",
       "       [10997.84496846],\n",
       "       [11147.88076614],\n",
       "       [11297.91656382],\n",
       "       [11447.9523615 ],\n",
       "       [11597.98815918],\n",
       "       [11748.02395686],\n",
       "       [11898.05975454],\n",
       "       [12048.09555222],\n",
       "       [12198.1313499 ],\n",
       "       [12348.16714757],\n",
       "       [12498.20294525],\n",
       "       [12648.23874293],\n",
       "       [12798.27454061],\n",
       "       [12948.31033829],\n",
       "       [13098.34613597],\n",
       "       [13248.38193365],\n",
       "       [13398.41773133],\n",
       "       [13548.45352901],\n",
       "       [13698.48932669],\n",
       "       [13848.52512437],\n",
       "       [13998.56092204],\n",
       "       [14148.59671972],\n",
       "       [14298.6325174 ],\n",
       "       [14448.66831508],\n",
       "       [14598.70411276],\n",
       "       [14748.73991044],\n",
       "       [14898.77570812],\n",
       "       [15048.8115058 ],\n",
       "       [15198.84730348],\n",
       "       [15348.88310116],\n",
       "       [15498.91889883],\n",
       "       [15648.95469651],\n",
       "       [15798.99049419],\n",
       "       [15949.02629187],\n",
       "       [16099.06208955],\n",
       "       [16249.09788723],\n",
       "       [16399.13368491],\n",
       "       [16549.16948259],\n",
       "       [16699.20528027],\n",
       "       [16849.24107795],\n",
       "       [16999.27687563],\n",
       "       [17149.3126733 ],\n",
       "       [17299.34847098],\n",
       "       [17449.38426866],\n",
       "       [17599.42006634],\n",
       "       [17749.45586402],\n",
       "       [17899.4916617 ],\n",
       "       [18049.52745938],\n",
       "       [18199.56325706],\n",
       "       [18349.59905474],\n",
       "       [18499.63485242],\n",
       "       [18649.67065009],\n",
       "       [18799.70644777],\n",
       "       [18949.74224545],\n",
       "       [19099.77804313],\n",
       "       [19249.81384081],\n",
       "       [19399.84963849],\n",
       "       [19549.88543617],\n",
       "       [19699.92123385],\n",
       "       [19849.95703153],\n",
       "       [19999.99282921],\n",
       "       [20150.02862689],\n",
       "       [20300.06442456],\n",
       "       [20450.10022224],\n",
       "       [20600.13601992],\n",
       "       [20750.1718176 ],\n",
       "       [20900.20761528],\n",
       "       [21050.24341296],\n",
       "       [21200.27921064],\n",
       "       [21350.31500832],\n",
       "       [21500.350806  ],\n",
       "       [21650.38660368],\n",
       "       [21800.42240135],\n",
       "       [21950.45819903],\n",
       "       [22100.49399671],\n",
       "       [22250.52979439],\n",
       "       [22400.56559207],\n",
       "       [22550.60138975],\n",
       "       [22700.63718743],\n",
       "       [22850.67298511],\n",
       "       [23000.70878279],\n",
       "       [23150.74458047],\n",
       "       [23300.78037814],\n",
       "       [23450.81617582],\n",
       "       [23600.8519735 ],\n",
       "       [23750.88777118],\n",
       "       [23900.92356886],\n",
       "       [24050.95936654],\n",
       "       [24200.99516422],\n",
       "       [24351.0309619 ],\n",
       "       [24501.06675958],\n",
       "       [24651.10255726],\n",
       "       [24801.13835494],\n",
       "       [24951.17415261],\n",
       "       [25101.20995029],\n",
       "       [25251.24574797],\n",
       "       [25401.28154565],\n",
       "       [25551.31734333],\n",
       "       [25701.35314101],\n",
       "       [25851.38893869],\n",
       "       [26001.42473637],\n",
       "       [26151.46053405],\n",
       "       [26301.49633173],\n",
       "       [26451.5321294 ],\n",
       "       [26601.56792708],\n",
       "       [26751.60372476],\n",
       "       [26901.63952244],\n",
       "       [27051.67532012],\n",
       "       [27201.7111178 ],\n",
       "       [27351.74691548],\n",
       "       [27501.78271316],\n",
       "       [27651.81851084],\n",
       "       [27801.85430852],\n",
       "       [27951.8901062 ],\n",
       "       [28101.92590387],\n",
       "       [28251.96170155],\n",
       "       [28401.99749923],\n",
       "       [28552.03329691],\n",
       "       [28702.06909459],\n",
       "       [28852.10489227],\n",
       "       [29002.14068995],\n",
       "       [29152.17648763],\n",
       "       [29302.21228531],\n",
       "       [29452.24808299],\n",
       "       [29602.28388066],\n",
       "       [29752.31967834],\n",
       "       [29902.35547602]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[15]\n",
      " [25]]\n",
      "Loss: 1.2939625376335508e+16\n",
      "EARLY STOP @ 1308\n"
     ]
    }
   ],
   "source": [
    "X_norm = min_max_normalize(X)\n",
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X_norm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   50.],\n",
       "       [29850.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 996, 995)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(), X.max(), X.max() - X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29850 / 995 = 30 \n",
    "# slope checks out\n",
    "\n",
    "# x - min. every single point is shifted over by the minimum value. line is shifted over by the slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X2 = wine[['density','alcohol']]\n",
    "X2_norm = min_max_normalize(X2)\n",
    "y2 = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-33.15237986168804, array([34.82170159,  0.39144139]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X2,y2)\n",
    "## Squared Error with sklearn.\n",
    "sum((lr.predict(X2) - y2)**2)\n",
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.55341784]\n",
      " [ 0.04632668]\n",
      " [ 0.10140291]]\n",
      "Loss: 5287551503614576.0\n",
      "Iteration: 10000\n",
      "Coef: [[0.22566935]\n",
      " [0.83438408]\n",
      " [0.43902616]]\n",
      "Loss: 816.584217720002\n",
      "Iteration: 20000\n",
      "Coef: [[0.48625587]\n",
      " [1.10121317]\n",
      " [0.38829796]]\n",
      "Loss: 806.863747729821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr2 \u001b[38;5;241m=\u001b[39m Linear_Regression(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m, init_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlr2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weight)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 136\u001b[0m, in \u001b[0;36mLinear_Regression.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m    135\u001b[0m dloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 136\u001b[0m dsquared_errors \u001b[38;5;241m=\u001b[39m dloss \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquared_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# dsquared_errors = dloss * 1\u001b[39;00m\n\u001b[1;32m    138\u001b[0m derrors \u001b[38;5;241m=\u001b[39m dsquared_errors \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m errors\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/core/numeric.py:266\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mReturn an array of ones with the same shape and type as a given array.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m res \u001b[38;5;241m=\u001b[39m empty_like(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, subok\u001b[38;5;241m=\u001b[39msubok, shape\u001b[38;5;241m=\u001b[39mshape)\n\u001b[0;32m--> 266\u001b[0m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr2 = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight=None)\n",
    "lr2.fit(X2, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52015526],\n",
       "       [1.13655104],\n",
       "       [0.38208525]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.55341784]\n",
      " [ 0.04632668]\n",
      " [ 0.10140291]]\n",
      "Loss: 1312507632361.2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10000\n",
      "Coef: [[2.23217576]\n",
      " [0.07111898]\n",
      " [5.0193558 ]]\n",
      "Loss: 805.8689303576189\n",
      "Iteration: 20000\n",
      "Coef: [[2.23211202]\n",
      " [0.09747445]\n",
      " [5.01935535]]\n",
      "Loss: 805.8683657160285\n",
      "Iteration: 30000\n",
      "Coef: [[2.23212022]\n",
      " [0.12382839]\n",
      " [5.0194034 ]]\n",
      "Loss: 805.8677993221213\n",
      "Iteration: 40000\n",
      "Coef: [[2.23205787]\n",
      " [0.15018001]\n",
      " [5.01940341]]\n",
      "Loss: 805.8672322634252\n",
      "Iteration: 50000\n",
      "Coef: [[2.23202368]\n",
      " [0.17654143]\n",
      " [5.0194226 ]]\n",
      "Loss: 805.8666666819033\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr2 \u001b[38;5;241m=\u001b[39m Linear_Regression(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m, init_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlr2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weight)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 162\u001b[0m, in \u001b[0;36mLinear_Regression.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m temp_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m dcoef\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# print(self.coef.shape, temp_coef.shape, (self.alpha * dcoef).shape)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# print(self.coef, temp_coef.shape)\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m temp_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_coef\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# technically double calculates, which doesn't need to be done. i'll fix this later.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m pre_error \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    166\u001b[0m current_error \u001b[38;5;241m=\u001b[39m temp_loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2172\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2173\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2179\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr2 = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight=None)\n",
    "lr2.fit(X2_norm, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 5000000)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((clf.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[1. 1.]\n",
      " [1. 2.]\n",
      " [1. 3.]]\n",
      "[[-0.55341784]\n",
      " [ 0.04632668]]\n",
      "Iteration: 0\n",
      "Coef: [[-0.53865325]\n",
      " [ 0.07967055]]\n",
      "Loss: 18.677391590518212\n",
      "EARLY STOP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.33083181e-15],\n",
       "       [ 1.00000000e+00]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tests \n",
    "\n",
    "test = Linear_Regression(intercept=True, verbose=True, alpha=1e-3)\n",
    "X2 = np.array([[1], [2], [3]])\n",
    "y2 = np.array([1, 2, 3])\n",
    "X2, y2\n",
    "test.fit(X2, y2)\n",
    "test.coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
