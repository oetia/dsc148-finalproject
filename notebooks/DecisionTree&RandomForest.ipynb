{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgqMGh9-gGm"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL FEATURES NUMERICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, max_depth = 1000, min_samples_split = 2, n_features = None, n_split = None):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "        # for random forest\n",
    "        # self.n_features = n_features\n",
    "        # self.n_split = n_split\n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        def __init__(self, feature_index_col: int, threshold: float, left: \"DecisionTree.Node\", right: \"DecisionTree.Node\", majority_class):\n",
    "            self.feature_index_col = feature_index_col\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.majority_class = majority_class # if a node is cut off due to max depth or size constraint, this becomes useful. also works for purity. \n",
    "            \n",
    "        def predict(self, x: np.ndarray):\n",
    "            if self.feature_index_col is None or self.threshold is None: # leaf node\n",
    "                return self.majority_class\n",
    "            else:\n",
    "                # print(self.feature_index_col, self.threshold, x[self.feature_index_col])\n",
    "                if x[self.feature_index_col] <= self.threshold:\n",
    "                    return self.left.predict(x)\n",
    "                else:\n",
    "                    return self.right.predict(x)\n",
    "    \n",
    "    \n",
    "    def entropy(self, y: np.ndarray):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        entropy = - (probs * np.log2(probs)).sum()\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "    def information_gain(self, X_col: np.ndarray, y: np.ndarray, threshold: float) -> float:\n",
    "        left_mask = X_col <= threshold # by convention left node meets condition\n",
    "        right_mask = ~left_mask\n",
    "        left_y = y[left_mask]\n",
    "        right_y = y[right_mask]\n",
    "        left_entropy = self.entropy(left_y)\n",
    "        right_entropy = self.entropy(right_y)\n",
    "        left_prob = left_y.shape[0] / y.shape[0]\n",
    "        right_prob = right_y.shape[0] / y.shape[0]\n",
    "        weighted_average_entropy = left_prob * left_entropy + right_prob * right_entropy\n",
    "        \n",
    "        total_entropy = self.entropy(y)\n",
    "        return total_entropy - weighted_average_entropy\n",
    "    \n",
    "    \n",
    "    def find_all_feature_split_rules(self, X: np.ndarray) -> List[np.ndarray]:\n",
    "        all_feature_split_rules = []\n",
    "        for feature_col in X.T:\n",
    "            unique_values = np.unique(feature_col) # also sorts values\n",
    "            unique_values_midpoints = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "            all_feature_split_rules.append(unique_values_midpoints)\n",
    "        return all_feature_split_rules\n",
    "\n",
    "    \n",
    "    def find_best_split(self, X: np.ndarray, y: np.ndarray, all_feature_split_rules: List[np.ndarray]) -> Tuple[int, float]:\n",
    "        features_best_splits = [] # for each feature track what was the best split\n",
    "        for index, feature_split_rules in enumerate(all_feature_split_rules):\n",
    "            X_col = X[:, index]\n",
    "            feature_best_split = (index, None, -1e9) # index, threshold, infogain\n",
    "            for threshold in feature_split_rules:\n",
    "                infogain = self.information_gain(X_col, y, threshold)\n",
    "                if infogain > feature_best_split[2]:\n",
    "                    feature_best_split = (index, threshold, infogain)\n",
    "            features_best_splits.append(feature_best_split)\n",
    "        \n",
    "        # find the best split across features\n",
    "        col_index, threshold, infogain = max(features_best_splits, key=lambda x: x[2])\n",
    "        \n",
    "        # if threshold is None: # happens when dataset has same X, but different Y.\n",
    "        #     print('threshold is None')\n",
    "        #     print(all_feature_split_rules)\n",
    "        #     print(X, y)\n",
    "            \n",
    "        return col_index, threshold\n",
    "    \n",
    "    \n",
    "    def build_tree(self, X: np.ndarray, y: np.ndarray, current_depth: int = 1):\n",
    "    \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = values[np.argmax(counts)]\n",
    "        # base cases: max depth exceeded, node too small, or pure node\n",
    "        if current_depth > self.max_depth or X.shape[0] < self.min_samples_split or len(np.unique(y)) == 1: \n",
    "            return DecisionTree.Node(None, None, None, None, majority_class)\n",
    "        \n",
    "        index_col, threshold = self.find_best_split(X, y, self.find_all_feature_split_rules(X))\n",
    "        \n",
    "        if threshold is None: # happens when X is all same value with different y\n",
    "            return DecisionTree.Node(None, None, None, None, majority_class)\n",
    "        \n",
    "        # print(index_col, threshold)\n",
    "        X_left = X[X[:, index_col] <= threshold]\n",
    "        y_left = y[X[:, index_col] <= threshold]\n",
    "        X_right = X[X[:, index_col] > threshold]\n",
    "        y_right = y[X[:, index_col] > threshold]\n",
    "        \n",
    "        # last node created is the root\n",
    "        left_node = self.build_tree(X_left, y_left, current_depth + 1)\n",
    "        right_node = self.build_tree(X_right, y_right, current_depth + 1)\n",
    "        \n",
    "        return DecisionTree.Node(index_col, threshold, left_node, right_node, majority_class)\n",
    "    \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.root = self.build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "            \n",
    "    def predict(self, dataset: np.ndarray):\n",
    "        result = []\n",
    "        for datapoint in dataset:\n",
    "            result.append(self.root.predict(datapoint))\n",
    "        return np.array(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine.drop(columns=[\"quality\"]).to_numpy()\n",
    "y = wine[\"quality\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.65,  4.8 ,  4.95,  5.05,  5.15,  5.25,  5.35,  5.45,  5.55,\n",
       "         5.65,  5.75,  5.85,  5.95,  6.05,  6.15,  6.25,  6.35,  6.45,\n",
       "         6.55,  6.65,  6.75,  6.85,  6.95,  7.05,  7.15,  7.25,  7.35,\n",
       "         7.45,  7.55,  7.65,  7.75,  7.85,  7.95,  8.05,  8.15,  8.25,\n",
       "         8.35,  8.45,  8.55,  8.65,  8.75,  8.85,  8.95,  9.05,  9.15,\n",
       "         9.25,  9.35,  9.45,  9.55,  9.65,  9.75,  9.85,  9.95, 10.05,\n",
       "        10.15, 10.25, 10.35, 10.45, 10.55, 10.65, 10.75, 10.85, 10.95,\n",
       "        11.05, 11.15, 11.25, 11.35, 11.45, 11.55, 11.65, 11.75, 11.85,\n",
       "        11.95, 12.05, 12.15, 12.25, 12.35, 12.45, 12.55, 12.65, 12.75,\n",
       "        12.85, 12.95, 13.1 , 13.25, 13.35, 13.45, 13.6 , 13.75, 13.9 ,\n",
       "        14.15, 14.65, 15.25, 15.55, 15.75]),\n",
       " array([0.14  , 0.17  , 0.185 , 0.195 , 0.205 , 0.215 , 0.225 , 0.235 ,\n",
       "        0.245 , 0.255 , 0.265 , 0.275 , 0.285 , 0.2925, 0.2975, 0.3025,\n",
       "        0.3075, 0.3125, 0.3175, 0.325 , 0.335 , 0.345 , 0.355 , 0.3625,\n",
       "        0.3675, 0.375 , 0.385 , 0.3925, 0.3975, 0.405 , 0.4125, 0.4175,\n",
       "        0.425 , 0.435 , 0.445 , 0.455 , 0.465 , 0.4725, 0.4775, 0.485 ,\n",
       "        0.495 , 0.505 , 0.515 , 0.525 , 0.535 , 0.5425, 0.5475, 0.555 ,\n",
       "        0.5625, 0.5675, 0.5725, 0.5775, 0.5825, 0.5875, 0.5925, 0.5975,\n",
       "        0.6025, 0.6075, 0.6125, 0.6175, 0.6225, 0.6275, 0.6325, 0.6375,\n",
       "        0.6425, 0.6475, 0.6525, 0.6575, 0.6625, 0.6675, 0.6725, 0.6775,\n",
       "        0.6825, 0.6875, 0.6925, 0.6975, 0.7025, 0.7075, 0.7125, 0.7175,\n",
       "        0.7225, 0.7275, 0.7325, 0.7375, 0.7425, 0.7475, 0.7525, 0.7575,\n",
       "        0.7625, 0.7675, 0.7725, 0.7775, 0.7825, 0.7875, 0.7925, 0.7975,\n",
       "        0.8025, 0.8075, 0.8125, 0.8175, 0.8225, 0.8275, 0.8325, 0.8375,\n",
       "        0.8425, 0.8475, 0.8525, 0.8575, 0.8625, 0.8675, 0.8725, 0.8775,\n",
       "        0.8825, 0.8875, 0.8925, 0.8975, 0.905 , 0.9125, 0.9175, 0.9275,\n",
       "        0.9425, 0.9525, 0.9575, 0.9625, 0.97  , 0.9775, 0.99  , 1.0025,\n",
       "        1.0075, 1.015 , 1.0225, 1.03  , 1.0375, 1.055 , 1.08  , 1.1025,\n",
       "        1.1225, 1.155 , 1.1825, 1.2125, 1.285 , 1.455 ]),\n",
       " array([0.005, 0.015, 0.025, 0.035, 0.045, 0.055, 0.065, 0.075, 0.085,\n",
       "        0.095, 0.105, 0.115, 0.125, 0.135, 0.145, 0.155, 0.165, 0.175,\n",
       "        0.185, 0.195, 0.205, 0.215, 0.225, 0.235, 0.245, 0.255, 0.265,\n",
       "        0.275, 0.285, 0.295, 0.305, 0.315, 0.325, 0.335, 0.345, 0.355,\n",
       "        0.365, 0.375, 0.385, 0.395, 0.405, 0.415, 0.425, 0.435, 0.445,\n",
       "        0.455, 0.465, 0.475, 0.485, 0.495, 0.505, 0.515, 0.525, 0.535,\n",
       "        0.545, 0.555, 0.565, 0.575, 0.585, 0.595, 0.605, 0.615, 0.625,\n",
       "        0.635, 0.645, 0.655, 0.665, 0.675, 0.685, 0.695, 0.705, 0.715,\n",
       "        0.725, 0.735, 0.745, 0.755, 0.77 , 0.785, 0.895]),\n",
       " array([ 1.05 ,  1.25 ,  1.35 ,  1.45 ,  1.55 ,  1.625,  1.675,  1.725,\n",
       "         1.775,  1.85 ,  1.95 ,  2.025,  2.075,  2.125,  2.175,  2.225,\n",
       "         2.275,  2.325,  2.375,  2.45 ,  2.525,  2.575,  2.625,  2.675,\n",
       "         2.75 ,  2.825,  2.875,  2.925,  2.975,  3.05 ,  3.15 ,  3.25 ,\n",
       "         3.35 ,  3.425,  3.475,  3.55 ,  3.625,  3.675,  3.725,  3.775,\n",
       "         3.85 ,  3.95 ,  4.05 ,  4.15 ,  4.225,  4.275,  4.35 ,  4.45 ,\n",
       "         4.55 ,  4.625,  4.675,  4.75 ,  4.9  ,  5.05 ,  5.125,  5.175,\n",
       "         5.3  ,  5.45 ,  5.55 ,  5.65 ,  5.75 ,  5.85 ,  5.95 ,  6.05 ,\n",
       "         6.15 ,  6.25 ,  6.35 ,  6.475,  6.575,  6.65 ,  6.85 ,  7.1  ,\n",
       "         7.25 ,  7.4  ,  7.65 ,  7.85 ,  8.   ,  8.2  ,  8.45 ,  8.7  ,\n",
       "         8.85 ,  8.95 ,  9.85 , 10.85 , 11.95 , 13.15 , 13.6  , 13.85 ,\n",
       "        14.65 , 15.45 ]),\n",
       " array([0.023 , 0.036 , 0.0385, 0.04  , 0.0415, 0.0425, 0.0435, 0.0445,\n",
       "        0.0455, 0.0465, 0.0475, 0.0485, 0.0495, 0.0505, 0.0515, 0.0525,\n",
       "        0.0535, 0.0545, 0.0555, 0.0565, 0.0575, 0.0585, 0.0595, 0.0605,\n",
       "        0.0615, 0.0625, 0.0635, 0.0645, 0.0655, 0.0665, 0.0675, 0.0685,\n",
       "        0.0695, 0.0705, 0.0715, 0.0725, 0.0735, 0.0745, 0.0755, 0.0765,\n",
       "        0.0775, 0.0785, 0.0795, 0.0805, 0.0815, 0.0825, 0.0835, 0.0845,\n",
       "        0.0855, 0.0865, 0.0875, 0.0885, 0.0895, 0.0905, 0.0915, 0.0925,\n",
       "        0.0935, 0.0945, 0.0955, 0.0965, 0.0975, 0.0985, 0.0995, 0.1005,\n",
       "        0.1015, 0.1025, 0.1035, 0.1045, 0.1055, 0.1065, 0.1075, 0.1085,\n",
       "        0.1095, 0.1105, 0.1115, 0.1125, 0.1135, 0.1145, 0.1155, 0.1165,\n",
       "        0.1175, 0.1185, 0.1195, 0.1205, 0.1215, 0.1225, 0.1235, 0.1245,\n",
       "        0.1255, 0.1265, 0.1275, 0.13  , 0.134 , 0.1365, 0.14  , 0.144 ,\n",
       "        0.1455, 0.1465, 0.1475, 0.15  , 0.1525, 0.155 , 0.158 , 0.16  ,\n",
       "        0.163 , 0.1655, 0.167 , 0.1685, 0.1695, 0.1705, 0.1715, 0.173 ,\n",
       "        0.175 , 0.177 , 0.182 , 0.188 , 0.192 , 0.197 , 0.2025, 0.209 ,\n",
       "        0.2135, 0.215 , 0.219 , 0.224 , 0.228 , 0.2325, 0.2355, 0.2385,\n",
       "        0.242 , 0.2465, 0.2565, 0.265 , 0.2685, 0.301 , 0.3345, 0.339 ,\n",
       "        0.342 , 0.3505, 0.359 , 0.364 , 0.3685, 0.378 , 0.394 , 0.402 ,\n",
       "        0.408 , 0.4135, 0.4145, 0.4185, 0.443 , 0.4655, 0.5385, 0.6105]),\n",
       " array([ 1.5 ,  2.5 ,  3.5 ,  4.5 ,  5.25,  5.75,  6.5 ,  7.5 ,  8.5 ,\n",
       "         9.5 , 10.5 , 11.5 , 12.5 , 13.5 , 14.5 , 15.5 , 16.5 , 17.5 ,\n",
       "        18.5 , 19.5 , 20.5 , 21.5 , 22.5 , 23.5 , 24.5 , 25.5 , 26.5 ,\n",
       "        27.5 , 28.5 , 29.5 , 30.5 , 31.5 , 32.5 , 33.5 , 34.5 , 35.5 ,\n",
       "        36.5 , 37.25, 37.75, 38.5 , 39.5 , 40.25, 40.75, 41.5 , 42.5 ,\n",
       "        44.  , 45.5 , 46.5 , 47.5 , 49.  , 50.5 , 51.5 , 52.5 , 53.5 ,\n",
       "        54.5 , 56.  , 61.5 , 67.  , 70.  ]),\n",
       " array([  6.5 ,   7.5 ,   8.5 ,   9.5 ,  10.5 ,  11.5 ,  12.5 ,  13.5 ,\n",
       "         14.5 ,  15.5 ,  16.5 ,  17.5 ,  18.5 ,  19.5 ,  20.5 ,  21.5 ,\n",
       "         22.5 ,  23.5 ,  24.5 ,  25.5 ,  26.5 ,  27.5 ,  28.5 ,  29.5 ,\n",
       "         30.5 ,  31.5 ,  32.5 ,  33.5 ,  34.5 ,  35.5 ,  36.5 ,  37.5 ,\n",
       "         38.5 ,  39.5 ,  40.5 ,  41.5 ,  42.5 ,  43.5 ,  44.5 ,  45.5 ,\n",
       "         46.5 ,  47.5 ,  48.5 ,  49.5 ,  50.5 ,  51.5 ,  52.5 ,  53.5 ,\n",
       "         54.5 ,  55.5 ,  56.5 ,  57.5 ,  58.5 ,  59.5 ,  60.5 ,  61.5 ,\n",
       "         62.5 ,  63.5 ,  64.5 ,  65.5 ,  66.5 ,  67.5 ,  68.5 ,  69.5 ,\n",
       "         70.5 ,  71.5 ,  72.5 ,  73.5 ,  74.5 ,  75.5 ,  76.5 ,  77.25,\n",
       "         77.75,  78.5 ,  79.5 ,  80.5 ,  81.5 ,  82.5 ,  83.5 ,  84.5 ,\n",
       "         85.5 ,  86.5 ,  87.5 ,  88.5 ,  89.5 ,  90.5 ,  91.5 ,  92.5 ,\n",
       "         93.5 ,  94.5 ,  95.5 ,  97.  ,  98.5 ,  99.5 , 100.5 , 101.5 ,\n",
       "        102.5 , 103.5 , 104.5 , 105.5 , 107.  , 108.5 , 109.5 , 110.5 ,\n",
       "        111.5 , 112.5 , 113.5 , 114.5 , 115.5 , 117.5 , 119.5 , 120.5 ,\n",
       "        121.5 , 123.  , 124.5 , 125.5 , 126.5 , 127.5 , 128.5 , 129.5 ,\n",
       "        130.5 , 132.  , 133.5 , 134.5 , 135.5 , 137.5 , 139.5 , 140.5 ,\n",
       "        141.5 , 142.5 , 143.5 , 144.5 , 146.  , 147.5 , 148.5 , 150.  ,\n",
       "        151.5 , 152.5 , 154.  , 157.5 , 162.5 , 221.5 , 283.5 ]),\n",
       " array([0.990135, 0.99042 , 0.99072 , 0.99082 , 0.99102 , 0.99135 ,\n",
       "        0.99152 , 0.991555, 0.991585, 0.99161 , 0.99166 , 0.99176 ,\n",
       "        0.991865, 0.992005, 0.99215 , 0.992275, 0.992355, 0.99238 ,\n",
       "        0.99241 , 0.99247 , 0.99254 , 0.99257 , 0.99261 , 0.99267 ,\n",
       "        0.99275 , 0.99283 , 0.99288 , 0.99291 , 0.99293 , 0.993   ,\n",
       "        0.9931  , 0.99315 , 0.99317 , 0.99319 , 0.99321 , 0.993225,\n",
       "        0.993255, 0.99329 , 0.993305, 0.993315, 0.99333 , 0.99335 ,\n",
       "        0.99338 , 0.993405, 0.993425, 0.99345 , 0.99347 , 0.99349 ,\n",
       "        0.99351 , 0.99353 , 0.99355 , 0.993565, 0.993575, 0.99359 ,\n",
       "        0.99361 , 0.99363 , 0.99367 , 0.993705, 0.993725, 0.99375 ,\n",
       "        0.99377 , 0.993785, 0.993795, 0.99382 , 0.993845, 0.993855,\n",
       "        0.993865, 0.993875, 0.9939  , 0.99393 , 0.993945, 0.993955,\n",
       "        0.993965, 0.993985, 0.99401 , 0.99405 , 0.99409 , 0.99412 ,\n",
       "        0.99415 , 0.994165, 0.994175, 0.994185, 0.994195, 0.994225,\n",
       "        0.994255, 0.99427 , 0.99429 , 0.99432 , 0.994355, 0.994375,\n",
       "        0.994385, 0.994395, 0.99442 , 0.99446 , 0.994495, 0.994525,\n",
       "        0.99455 , 0.99457 , 0.994585, 0.994595, 0.99461 , 0.99463 ,\n",
       "        0.994655, 0.994675, 0.99469 , 0.994705, 0.994715, 0.994725,\n",
       "        0.994735, 0.99475 , 0.99477 , 0.994785, 0.994795, 0.994815,\n",
       "        0.994835, 0.99485 , 0.99487 , 0.994885, 0.994895, 0.994905,\n",
       "        0.994915, 0.99493 , 0.994945, 0.994955, 0.99497 , 0.994985,\n",
       "        0.994995, 0.995005, 0.995015, 0.99503 , 0.99505 , 0.99507 ,\n",
       "        0.995085, 0.995095, 0.99511 , 0.99513 , 0.99515 , 0.995165,\n",
       "        0.995175, 0.995185, 0.995195, 0.995205, 0.995215, 0.995225,\n",
       "        0.995235, 0.995245, 0.995255, 0.99527 , 0.995285, 0.995295,\n",
       "        0.995305, 0.995315, 0.995325, 0.995335, 0.99535 , 0.99537 ,\n",
       "        0.99539 , 0.995405, 0.995415, 0.995425, 0.995435, 0.995445,\n",
       "        0.995455, 0.995465, 0.99548 , 0.995495, 0.995505, 0.995515,\n",
       "        0.995525, 0.995535, 0.995545, 0.995555, 0.995565, 0.995575,\n",
       "        0.99559 , 0.99561 , 0.99563 , 0.995645, 0.995655, 0.99567 ,\n",
       "        0.995685, 0.995695, 0.99571 , 0.995725, 0.995735, 0.995745,\n",
       "        0.995755, 0.995765, 0.995775, 0.99579 , 0.995805, 0.995815,\n",
       "        0.99583 , 0.995845, 0.995855, 0.995865, 0.995875, 0.995885,\n",
       "        0.995895, 0.99591 , 0.995925, 0.995935, 0.99595 , 0.99597 ,\n",
       "        0.995985, 0.995995, 0.996015, 0.996035, 0.996045, 0.996055,\n",
       "        0.99607 , 0.996085, 0.996095, 0.99611 , 0.996125, 0.996135,\n",
       "        0.996145, 0.996155, 0.996165, 0.99618 , 0.996195, 0.996205,\n",
       "        0.996215, 0.996225, 0.996235, 0.996245, 0.99626 , 0.996275,\n",
       "        0.996285, 0.996295, 0.996305, 0.996315, 0.996325, 0.996335,\n",
       "        0.996345, 0.996355, 0.99637 , 0.996385, 0.996395, 0.996405,\n",
       "        0.996415, 0.996425, 0.99644 , 0.996455, 0.996465, 0.996475,\n",
       "        0.996485, 0.996495, 0.996505, 0.996515, 0.99653 , 0.996545,\n",
       "        0.996555, 0.99657 , 0.996585, 0.996595, 0.996605, 0.996625,\n",
       "        0.996645, 0.996655, 0.996665, 0.996675, 0.996685, 0.996695,\n",
       "        0.99671 , 0.99673 , 0.996745, 0.996755, 0.996765, 0.996775,\n",
       "        0.99679 , 0.99681 , 0.996825, 0.996835, 0.996845, 0.996855,\n",
       "        0.99687 , 0.996885, 0.996895, 0.99691 , 0.996925, 0.996935,\n",
       "        0.996945, 0.99696 , 0.996975, 0.996985, 0.996995, 0.997005,\n",
       "        0.997015, 0.99703 , 0.997045, 0.997055, 0.99707 , 0.997085,\n",
       "        0.997095, 0.99711 , 0.997125, 0.997135, 0.99715 , 0.997165,\n",
       "        0.997175, 0.997185, 0.997195, 0.997205, 0.997215, 0.99723 ,\n",
       "        0.997245, 0.997255, 0.997265, 0.997275, 0.997285, 0.997295,\n",
       "        0.99731 , 0.997325, 0.997335, 0.997345, 0.997355, 0.99737 ,\n",
       "        0.997385, 0.997395, 0.997415, 0.997435, 0.997445, 0.997455,\n",
       "        0.997465, 0.997475, 0.99749 , 0.99751 , 0.99753 , 0.99755 ,\n",
       "        0.99757 , 0.99759 , 0.997605, 0.997625, 0.997645, 0.997665,\n",
       "        0.997685, 0.997695, 0.99771 , 0.99773 , 0.997765, 0.997795,\n",
       "        0.99781 , 0.997825, 0.997835, 0.997845, 0.997855, 0.997865,\n",
       "        0.997875, 0.99789 , 0.997905, 0.997935, 0.99797 , 0.99799 ,\n",
       "        0.998005, 0.99802 , 0.998055, 0.99809 , 0.99812 , 0.998145,\n",
       "        0.99816 , 0.998175, 0.99819 , 0.99821 , 0.998225, 0.998235,\n",
       "        0.99826 , 0.99829 , 0.99831 , 0.99833 , 0.99835 , 0.99838 ,\n",
       "        0.99841 , 0.998435, 0.998475, 0.99851 , 0.99853 , 0.998545,\n",
       "        0.99857 , 0.998595, 0.99862 , 0.998645, 0.998675, 0.99874 ,\n",
       "        0.99879 , 0.99884 , 0.99889 , 0.99891 , 0.99896 , 0.999005,\n",
       "        0.999055, 0.99912 , 0.999145, 0.999165, 0.99919 , 0.99921 ,\n",
       "        0.999235, 0.999275, 0.999325, 0.999365, 0.999385, 0.999395,\n",
       "        0.99945 , 0.99955 , 0.999625, 0.999675, 0.99972 , 0.999745,\n",
       "        0.999755, 0.99978 , 0.99985 , 0.99995 , 1.000025, 1.000075,\n",
       "        1.00011 , 1.000135, 1.000175, 1.00022 , 1.000245, 1.000275,\n",
       "        1.00035 , 1.0005  , 1.0007  , 1.0009  , 1.0012  , 1.00145 ,\n",
       "        1.00165 , 1.00195 , 1.00215 , 1.00231 , 1.00251 , 1.002745,\n",
       "        1.00302 , 1.003175, 1.003445]),\n",
       " array([2.8  , 2.865, 2.875, 2.885, 2.895, 2.91 , 2.925, 2.935, 2.945,\n",
       "        2.965, 2.985, 2.995, 3.005, 3.015, 3.025, 3.035, 3.045, 3.055,\n",
       "        3.065, 3.075, 3.085, 3.095, 3.105, 3.115, 3.125, 3.135, 3.145,\n",
       "        3.155, 3.165, 3.175, 3.185, 3.195, 3.205, 3.215, 3.225, 3.235,\n",
       "        3.245, 3.255, 3.265, 3.275, 3.285, 3.295, 3.305, 3.315, 3.325,\n",
       "        3.335, 3.345, 3.355, 3.365, 3.375, 3.385, 3.395, 3.405, 3.415,\n",
       "        3.425, 3.435, 3.445, 3.455, 3.465, 3.475, 3.485, 3.495, 3.505,\n",
       "        3.515, 3.525, 3.535, 3.545, 3.555, 3.565, 3.575, 3.585, 3.595,\n",
       "        3.605, 3.615, 3.625, 3.645, 3.665, 3.675, 3.685, 3.695, 3.705,\n",
       "        3.715, 3.73 , 3.745, 3.765, 3.815, 3.875, 3.955]),\n",
       " array([0.35 , 0.38 , 0.395, 0.41 , 0.425, 0.435, 0.445, 0.455, 0.465,\n",
       "        0.475, 0.485, 0.495, 0.505, 0.515, 0.525, 0.535, 0.545, 0.555,\n",
       "        0.565, 0.575, 0.585, 0.595, 0.605, 0.615, 0.625, 0.635, 0.645,\n",
       "        0.655, 0.665, 0.675, 0.685, 0.695, 0.705, 0.715, 0.725, 0.735,\n",
       "        0.745, 0.755, 0.765, 0.775, 0.785, 0.795, 0.805, 0.815, 0.825,\n",
       "        0.835, 0.845, 0.855, 0.865, 0.875, 0.885, 0.895, 0.905, 0.915,\n",
       "        0.925, 0.935, 0.945, 0.955, 0.965, 0.975, 0.985, 0.995, 1.005,\n",
       "        1.015, 1.025, 1.035, 1.045, 1.055, 1.065, 1.075, 1.085, 1.095,\n",
       "        1.105, 1.115, 1.125, 1.135, 1.145, 1.155, 1.165, 1.175, 1.19 ,\n",
       "        1.21 , 1.24 , 1.27 , 1.295, 1.32 , 1.335, 1.35 , 1.46 , 1.575,\n",
       "        1.6  , 1.615, 1.785, 1.965, 1.99 ]),\n",
       " array([ 8.45      ,  8.6       ,  8.75      ,  8.9       ,  9.025     ,\n",
       "         9.075     ,  9.15      ,  9.21666667,  9.24166667,  9.275     ,\n",
       "         9.35      ,  9.45      ,  9.525     ,  9.55833333,  9.58333333,\n",
       "         9.65      ,  9.75      ,  9.85      ,  9.925     ,  9.975     ,\n",
       "        10.01666667, 10.06666667, 10.15      , 10.25      , 10.35      ,\n",
       "        10.45      , 10.525     , 10.575     , 10.65      , 10.725     ,\n",
       "        10.775     , 10.85      , 10.95      , 11.03333333, 11.08333333,\n",
       "        11.15      , 11.25      , 11.35      , 11.45      , 11.55      ,\n",
       "        11.65      , 11.75      , 11.85      , 11.925     , 11.975     ,\n",
       "        12.05      , 12.15      , 12.25      , 12.35      , 12.45      ,\n",
       "        12.55      , 12.65      , 12.75      , 12.85      , 12.95      ,\n",
       "        13.05      , 13.15      , 13.25      , 13.35      , 13.45      ,\n",
       "        13.53333333, 13.58333333, 13.8       , 14.45      ])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_rules = []\n",
    "for feature_col in X_np.T:\n",
    "    unique_values = np.unique(feature_col) # sorts unique values\n",
    "    unique_values_midpoints = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "    split_rules.append(unique_values_midpoints)\n",
    "    \n",
    "split_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_splits(X: np.ndarray) -> List[np.ndarray]:\n",
    "    split_rules = []\n",
    "    for feature_col in X.T:\n",
    "        unique_values = np.unique(feature_col) # sorts unique values\n",
    "        unique_values_midpoints = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "        split_rules.append(unique_values_midpoints)\n",
    "    return split_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7090616375059242"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(y_np, return_counts=True)\n",
    "probs = counts / counts.sum()\n",
    "entropy = - (probs * np.log2(probs)).sum()\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: np.ndarray):\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    entropy = - (probs * np.log2(probs)).sum()\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "1.0\n",
      "0.9709505944546686\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "# Test case 1: All elements are of the same class\n",
    "test_case_1 = np.array([0, 0, 0, 0, 0])\n",
    "print(entropy(test_case_1)) # Expected output: 0.0\n",
    "# Test case 2: Elements are evenly split between two classes\n",
    "test_case_2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "print(entropy(test_case_2)) # Expected output: 1.0\n",
    "# Test case 3: Elements are unevenly split between two classes\n",
    "test_case_3 = np.array([0, 0, 0, 1, 1])\n",
    "print(entropy(test_case_3)) # Expected output: 0.971\n",
    "# Test case 4: Only one element\n",
    "test_case_4 = np.array([0])\n",
    "print(entropy(test_case_4)) # Expected output: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X_col: np.ndarray, y: np.ndarray, threshold: float) -> float:\n",
    "    left_mask = X_col <= threshold # by convention left node meets condition\n",
    "    right_mask = ~left_mask\n",
    "    left_y = y[left_mask]\n",
    "    right_y = y[right_mask]\n",
    "    left_entropy = entropy(left_y)\n",
    "    right_entropy = entropy(right_y)\n",
    "    left_prob = left_y.shape[0] / y.shape[0]\n",
    "    right_prob = right_y.shape[0] / y.shape[0]\n",
    "    weighted_average_entropy = left_prob * left_entropy + right_prob * right_entropy\n",
    "    \n",
    "    total_entropy = entropy(y)\n",
    "    return total_entropy - weighted_average_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.65  4.8   4.95  5.05  5.15  5.25  5.35  5.45  5.55  5.65  5.75  5.85\n",
      "  5.95  6.05  6.15  6.25  6.35  6.45  6.55  6.65  6.75  6.85  6.95  7.05\n",
      "  7.15  7.25  7.35  7.45  7.55  7.65  7.75  7.85  7.95  8.05  8.15  8.25\n",
      "  8.35  8.45  8.55  8.65  8.75  8.85  8.95  9.05  9.15  9.25  9.35  9.45\n",
      "  9.55  9.65  9.75  9.85  9.95 10.05 10.15 10.25 10.35 10.45 10.55 10.65\n",
      " 10.75 10.85 10.95 11.05 11.15 11.25 11.35 11.45 11.55 11.65 11.75 11.85\n",
      " 11.95 12.05 12.15 12.25 12.35 12.45 12.55 12.65 12.75 12.85 12.95 13.1\n",
      " 13.25 13.35 13.45 13.6  13.75 13.9  14.15 14.65 15.25 15.55 15.75]\n",
      "[0.14   0.17   0.185  0.195  0.205  0.215  0.225  0.235  0.245  0.255\n",
      " 0.265  0.275  0.285  0.2925 0.2975 0.3025 0.3075 0.3125 0.3175 0.325\n",
      " 0.335  0.345  0.355  0.3625 0.3675 0.375  0.385  0.3925 0.3975 0.405\n",
      " 0.4125 0.4175 0.425  0.435  0.445  0.455  0.465  0.4725 0.4775 0.485\n",
      " 0.495  0.505  0.515  0.525  0.535  0.5425 0.5475 0.555  0.5625 0.5675\n",
      " 0.5725 0.5775 0.5825 0.5875 0.5925 0.5975 0.6025 0.6075 0.6125 0.6175\n",
      " 0.6225 0.6275 0.6325 0.6375 0.6425 0.6475 0.6525 0.6575 0.6625 0.6675\n",
      " 0.6725 0.6775 0.6825 0.6875 0.6925 0.6975 0.7025 0.7075 0.7125 0.7175\n",
      " 0.7225 0.7275 0.7325 0.7375 0.7425 0.7475 0.7525 0.7575 0.7625 0.7675\n",
      " 0.7725 0.7775 0.7825 0.7875 0.7925 0.7975 0.8025 0.8075 0.8125 0.8175\n",
      " 0.8225 0.8275 0.8325 0.8375 0.8425 0.8475 0.8525 0.8575 0.8625 0.8675\n",
      " 0.8725 0.8775 0.8825 0.8875 0.8925 0.8975 0.905  0.9125 0.9175 0.9275\n",
      " 0.9425 0.9525 0.9575 0.9625 0.97   0.9775 0.99   1.0025 1.0075 1.015\n",
      " 1.0225 1.03   1.0375 1.055  1.08   1.1025 1.1225 1.155  1.1825 1.2125\n",
      " 1.285  1.455 ]\n",
      "[0.005 0.015 0.025 0.035 0.045 0.055 0.065 0.075 0.085 0.095 0.105 0.115\n",
      " 0.125 0.135 0.145 0.155 0.165 0.175 0.185 0.195 0.205 0.215 0.225 0.235\n",
      " 0.245 0.255 0.265 0.275 0.285 0.295 0.305 0.315 0.325 0.335 0.345 0.355\n",
      " 0.365 0.375 0.385 0.395 0.405 0.415 0.425 0.435 0.445 0.455 0.465 0.475\n",
      " 0.485 0.495 0.505 0.515 0.525 0.535 0.545 0.555 0.565 0.575 0.585 0.595\n",
      " 0.605 0.615 0.625 0.635 0.645 0.655 0.665 0.675 0.685 0.695 0.705 0.715\n",
      " 0.725 0.735 0.745 0.755 0.77  0.785 0.895]\n",
      "[ 1.05   1.25   1.35   1.45   1.55   1.625  1.675  1.725  1.775  1.85\n",
      "  1.95   2.025  2.075  2.125  2.175  2.225  2.275  2.325  2.375  2.45\n",
      "  2.525  2.575  2.625  2.675  2.75   2.825  2.875  2.925  2.975  3.05\n",
      "  3.15   3.25   3.35   3.425  3.475  3.55   3.625  3.675  3.725  3.775\n",
      "  3.85   3.95   4.05   4.15   4.225  4.275  4.35   4.45   4.55   4.625\n",
      "  4.675  4.75   4.9    5.05   5.125  5.175  5.3    5.45   5.55   5.65\n",
      "  5.75   5.85   5.95   6.05   6.15   6.25   6.35   6.475  6.575  6.65\n",
      "  6.85   7.1    7.25   7.4    7.65   7.85   8.     8.2    8.45   8.7\n",
      "  8.85   8.95   9.85  10.85  11.95  13.15  13.6   13.85  14.65  15.45 ]\n",
      "[0.023  0.036  0.0385 0.04   0.0415 0.0425 0.0435 0.0445 0.0455 0.0465\n",
      " 0.0475 0.0485 0.0495 0.0505 0.0515 0.0525 0.0535 0.0545 0.0555 0.0565\n",
      " 0.0575 0.0585 0.0595 0.0605 0.0615 0.0625 0.0635 0.0645 0.0655 0.0665\n",
      " 0.0675 0.0685 0.0695 0.0705 0.0715 0.0725 0.0735 0.0745 0.0755 0.0765\n",
      " 0.0775 0.0785 0.0795 0.0805 0.0815 0.0825 0.0835 0.0845 0.0855 0.0865\n",
      " 0.0875 0.0885 0.0895 0.0905 0.0915 0.0925 0.0935 0.0945 0.0955 0.0965\n",
      " 0.0975 0.0985 0.0995 0.1005 0.1015 0.1025 0.1035 0.1045 0.1055 0.1065\n",
      " 0.1075 0.1085 0.1095 0.1105 0.1115 0.1125 0.1135 0.1145 0.1155 0.1165\n",
      " 0.1175 0.1185 0.1195 0.1205 0.1215 0.1225 0.1235 0.1245 0.1255 0.1265\n",
      " 0.1275 0.13   0.134  0.1365 0.14   0.144  0.1455 0.1465 0.1475 0.15\n",
      " 0.1525 0.155  0.158  0.16   0.163  0.1655 0.167  0.1685 0.1695 0.1705\n",
      " 0.1715 0.173  0.175  0.177  0.182  0.188  0.192  0.197  0.2025 0.209\n",
      " 0.2135 0.215  0.219  0.224  0.228  0.2325 0.2355 0.2385 0.242  0.2465\n",
      " 0.2565 0.265  0.2685 0.301  0.3345 0.339  0.342  0.3505 0.359  0.364\n",
      " 0.3685 0.378  0.394  0.402  0.408  0.4135 0.4145 0.4185 0.443  0.4655\n",
      " 0.5385 0.6105]\n",
      "[ 1.5   2.5   3.5   4.5   5.25  5.75  6.5   7.5   8.5   9.5  10.5  11.5\n",
      " 12.5  13.5  14.5  15.5  16.5  17.5  18.5  19.5  20.5  21.5  22.5  23.5\n",
      " 24.5  25.5  26.5  27.5  28.5  29.5  30.5  31.5  32.5  33.5  34.5  35.5\n",
      " 36.5  37.25 37.75 38.5  39.5  40.25 40.75 41.5  42.5  44.   45.5  46.5\n",
      " 47.5  49.   50.5  51.5  52.5  53.5  54.5  56.   61.5  67.   70.  ]\n",
      "[  6.5    7.5    8.5    9.5   10.5   11.5   12.5   13.5   14.5   15.5\n",
      "  16.5   17.5   18.5   19.5   20.5   21.5   22.5   23.5   24.5   25.5\n",
      "  26.5   27.5   28.5   29.5   30.5   31.5   32.5   33.5   34.5   35.5\n",
      "  36.5   37.5   38.5   39.5   40.5   41.5   42.5   43.5   44.5   45.5\n",
      "  46.5   47.5   48.5   49.5   50.5   51.5   52.5   53.5   54.5   55.5\n",
      "  56.5   57.5   58.5   59.5   60.5   61.5   62.5   63.5   64.5   65.5\n",
      "  66.5   67.5   68.5   69.5   70.5   71.5   72.5   73.5   74.5   75.5\n",
      "  76.5   77.25  77.75  78.5   79.5   80.5   81.5   82.5   83.5   84.5\n",
      "  85.5   86.5   87.5   88.5   89.5   90.5   91.5   92.5   93.5   94.5\n",
      "  95.5   97.    98.5   99.5  100.5  101.5  102.5  103.5  104.5  105.5\n",
      " 107.   108.5  109.5  110.5  111.5  112.5  113.5  114.5  115.5  117.5\n",
      " 119.5  120.5  121.5  123.   124.5  125.5  126.5  127.5  128.5  129.5\n",
      " 130.5  132.   133.5  134.5  135.5  137.5  139.5  140.5  141.5  142.5\n",
      " 143.5  144.5  146.   147.5  148.5  150.   151.5  152.5  154.   157.5\n",
      " 162.5  221.5  283.5 ]\n",
      "[0.990135 0.99042  0.99072  0.99082  0.99102  0.99135  0.99152  0.991555\n",
      " 0.991585 0.99161  0.99166  0.99176  0.991865 0.992005 0.99215  0.992275\n",
      " 0.992355 0.99238  0.99241  0.99247  0.99254  0.99257  0.99261  0.99267\n",
      " 0.99275  0.99283  0.99288  0.99291  0.99293  0.993    0.9931   0.99315\n",
      " 0.99317  0.99319  0.99321  0.993225 0.993255 0.99329  0.993305 0.993315\n",
      " 0.99333  0.99335  0.99338  0.993405 0.993425 0.99345  0.99347  0.99349\n",
      " 0.99351  0.99353  0.99355  0.993565 0.993575 0.99359  0.99361  0.99363\n",
      " 0.99367  0.993705 0.993725 0.99375  0.99377  0.993785 0.993795 0.99382\n",
      " 0.993845 0.993855 0.993865 0.993875 0.9939   0.99393  0.993945 0.993955\n",
      " 0.993965 0.993985 0.99401  0.99405  0.99409  0.99412  0.99415  0.994165\n",
      " 0.994175 0.994185 0.994195 0.994225 0.994255 0.99427  0.99429  0.99432\n",
      " 0.994355 0.994375 0.994385 0.994395 0.99442  0.99446  0.994495 0.994525\n",
      " 0.99455  0.99457  0.994585 0.994595 0.99461  0.99463  0.994655 0.994675\n",
      " 0.99469  0.994705 0.994715 0.994725 0.994735 0.99475  0.99477  0.994785\n",
      " 0.994795 0.994815 0.994835 0.99485  0.99487  0.994885 0.994895 0.994905\n",
      " 0.994915 0.99493  0.994945 0.994955 0.99497  0.994985 0.994995 0.995005\n",
      " 0.995015 0.99503  0.99505  0.99507  0.995085 0.995095 0.99511  0.99513\n",
      " 0.99515  0.995165 0.995175 0.995185 0.995195 0.995205 0.995215 0.995225\n",
      " 0.995235 0.995245 0.995255 0.99527  0.995285 0.995295 0.995305 0.995315\n",
      " 0.995325 0.995335 0.99535  0.99537  0.99539  0.995405 0.995415 0.995425\n",
      " 0.995435 0.995445 0.995455 0.995465 0.99548  0.995495 0.995505 0.995515\n",
      " 0.995525 0.995535 0.995545 0.995555 0.995565 0.995575 0.99559  0.99561\n",
      " 0.99563  0.995645 0.995655 0.99567  0.995685 0.995695 0.99571  0.995725\n",
      " 0.995735 0.995745 0.995755 0.995765 0.995775 0.99579  0.995805 0.995815\n",
      " 0.99583  0.995845 0.995855 0.995865 0.995875 0.995885 0.995895 0.99591\n",
      " 0.995925 0.995935 0.99595  0.99597  0.995985 0.995995 0.996015 0.996035\n",
      " 0.996045 0.996055 0.99607  0.996085 0.996095 0.99611  0.996125 0.996135\n",
      " 0.996145 0.996155 0.996165 0.99618  0.996195 0.996205 0.996215 0.996225\n",
      " 0.996235 0.996245 0.99626  0.996275 0.996285 0.996295 0.996305 0.996315\n",
      " 0.996325 0.996335 0.996345 0.996355 0.99637  0.996385 0.996395 0.996405\n",
      " 0.996415 0.996425 0.99644  0.996455 0.996465 0.996475 0.996485 0.996495\n",
      " 0.996505 0.996515 0.99653  0.996545 0.996555 0.99657  0.996585 0.996595\n",
      " 0.996605 0.996625 0.996645 0.996655 0.996665 0.996675 0.996685 0.996695\n",
      " 0.99671  0.99673  0.996745 0.996755 0.996765 0.996775 0.99679  0.99681\n",
      " 0.996825 0.996835 0.996845 0.996855 0.99687  0.996885 0.996895 0.99691\n",
      " 0.996925 0.996935 0.996945 0.99696  0.996975 0.996985 0.996995 0.997005\n",
      " 0.997015 0.99703  0.997045 0.997055 0.99707  0.997085 0.997095 0.99711\n",
      " 0.997125 0.997135 0.99715  0.997165 0.997175 0.997185 0.997195 0.997205\n",
      " 0.997215 0.99723  0.997245 0.997255 0.997265 0.997275 0.997285 0.997295\n",
      " 0.99731  0.997325 0.997335 0.997345 0.997355 0.99737  0.997385 0.997395\n",
      " 0.997415 0.997435 0.997445 0.997455 0.997465 0.997475 0.99749  0.99751\n",
      " 0.99753  0.99755  0.99757  0.99759  0.997605 0.997625 0.997645 0.997665\n",
      " 0.997685 0.997695 0.99771  0.99773  0.997765 0.997795 0.99781  0.997825\n",
      " 0.997835 0.997845 0.997855 0.997865 0.997875 0.99789  0.997905 0.997935\n",
      " 0.99797  0.99799  0.998005 0.99802  0.998055 0.99809  0.99812  0.998145\n",
      " 0.99816  0.998175 0.99819  0.99821  0.998225 0.998235 0.99826  0.99829\n",
      " 0.99831  0.99833  0.99835  0.99838  0.99841  0.998435 0.998475 0.99851\n",
      " 0.99853  0.998545 0.99857  0.998595 0.99862  0.998645 0.998675 0.99874\n",
      " 0.99879  0.99884  0.99889  0.99891  0.99896  0.999005 0.999055 0.99912\n",
      " 0.999145 0.999165 0.99919  0.99921  0.999235 0.999275 0.999325 0.999365\n",
      " 0.999385 0.999395 0.99945  0.99955  0.999625 0.999675 0.99972  0.999745\n",
      " 0.999755 0.99978  0.99985  0.99995  1.000025 1.000075 1.00011  1.000135\n",
      " 1.000175 1.00022  1.000245 1.000275 1.00035  1.0005   1.0007   1.0009\n",
      " 1.0012   1.00145  1.00165  1.00195  1.00215  1.00231  1.00251  1.002745\n",
      " 1.00302  1.003175 1.003445]\n",
      "[2.8   2.865 2.875 2.885 2.895 2.91  2.925 2.935 2.945 2.965 2.985 2.995\n",
      " 3.005 3.015 3.025 3.035 3.045 3.055 3.065 3.075 3.085 3.095 3.105 3.115\n",
      " 3.125 3.135 3.145 3.155 3.165 3.175 3.185 3.195 3.205 3.215 3.225 3.235\n",
      " 3.245 3.255 3.265 3.275 3.285 3.295 3.305 3.315 3.325 3.335 3.345 3.355\n",
      " 3.365 3.375 3.385 3.395 3.405 3.415 3.425 3.435 3.445 3.455 3.465 3.475\n",
      " 3.485 3.495 3.505 3.515 3.525 3.535 3.545 3.555 3.565 3.575 3.585 3.595\n",
      " 3.605 3.615 3.625 3.645 3.665 3.675 3.685 3.695 3.705 3.715 3.73  3.745\n",
      " 3.765 3.815 3.875 3.955]\n",
      "[0.35  0.38  0.395 0.41  0.425 0.435 0.445 0.455 0.465 0.475 0.485 0.495\n",
      " 0.505 0.515 0.525 0.535 0.545 0.555 0.565 0.575 0.585 0.595 0.605 0.615\n",
      " 0.625 0.635 0.645 0.655 0.665 0.675 0.685 0.695 0.705 0.715 0.725 0.735\n",
      " 0.745 0.755 0.765 0.775 0.785 0.795 0.805 0.815 0.825 0.835 0.845 0.855\n",
      " 0.865 0.875 0.885 0.895 0.905 0.915 0.925 0.935 0.945 0.955 0.965 0.975\n",
      " 0.985 0.995 1.005 1.015 1.025 1.035 1.045 1.055 1.065 1.075 1.085 1.095\n",
      " 1.105 1.115 1.125 1.135 1.145 1.155 1.165 1.175 1.19  1.21  1.24  1.27\n",
      " 1.295 1.32  1.335 1.35  1.46  1.575 1.6   1.615 1.785 1.965 1.99 ]\n",
      "[ 8.45        8.6         8.75        8.9         9.025       9.075\n",
      "  9.15        9.21666667  9.24166667  9.275       9.35        9.45\n",
      "  9.525       9.55833333  9.58333333  9.65        9.75        9.85\n",
      "  9.925       9.975      10.01666667 10.06666667 10.15       10.25\n",
      " 10.35       10.45       10.525      10.575      10.65       10.725\n",
      " 10.775      10.85       10.95       11.03333333 11.08333333 11.15\n",
      " 11.25       11.35       11.45       11.55       11.65       11.75\n",
      " 11.85       11.925      11.975      12.05       12.15       12.25\n",
      " 12.35       12.45       12.55       12.65       12.75       12.85\n",
      " 12.95       13.05       13.15       13.25       13.35       13.45\n",
      " 13.53333333 13.58333333 13.8        14.45      ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, feature_split_rules in enumerate(split_rules):\n",
    "    X_col = X_np[:, index]\n",
    "    for splitting_rule in feature_split_rules:\n",
    "        left_mask = X_col <= splitting_rule # by convention left node meets condition\n",
    "        right_mask = ~left_mask\n",
    "        left_y = y_np[left_mask]\n",
    "        right_y = y_np[right_mask]\n",
    "        \n",
    "        left_entropy = entropy(left_y)\n",
    "        right_entropy = entropy(right_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best split\n",
    "all_feature_split_rules = split_rules\n",
    "\n",
    "def find_best_split(X: np.ndarray, y: np.ndarray, all_feature_split_rules: List[np.ndarray]) -> Tuple[int, float]:\n",
    "    \n",
    "    features_best_splits = []\n",
    "    for index, feature_split_rules in enumerate(all_feature_split_rules):\n",
    "        X_col = X[:, index]\n",
    "        feature_best_split = (index, None, -1e9) # index, threshold, infogain\n",
    "        for threshold in feature_split_rules:\n",
    "            infogain = information_gain(X_col, y, threshold)\n",
    "            if infogain > feature_best_split[2]:\n",
    "                feature_best_split = (index, threshold, infogain)\n",
    "        features_best_splits.append(feature_best_split)\n",
    "        \n",
    "    col_index, threshold, infogain = max(features_best_splits, key=lambda x: x[2])\n",
    "    return col_index, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x134fd6a10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1YElEQVR4nO3deXwU9eH/8fdnd7Obg2TlkCMFBBWLIiCKqOBZqdRaq7ZqUVTQFr/fij9FWqtYz1aNYO3X4gFCq1IvbPsVr2+pByrWm4IoWsQLhYoBFciG3Jn5/P7YEAjkotn9jB1ez8djSzMzybz5ZJx578zsYKy1VgAAAI5Egg4AAAB2LZQPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE7Fgg6wPd/3tXbtWhUWFsoYE3QcAADQDtZalZeXq7i4WJFI6+c2vnblY+3aterTp0/QMQAAwL9hzZo16t27d6vLfO3KR2FhoaR0+KKiooDTAACA9kilUurTp0/jcbw1X7vyseVSS1FREeUDAID/MO25ZYIbTgEAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOfe0eMpYttv5jqW65pKgUP1Qm2i3oSKFkvfVS7RuSPClnqEysX9CRQsn6m6XalyVbKcX2lmL7828hZUFdbZ2WPrtcZV+ktHufrhpy1H6KRqNBxwod3/f1zkvvqXTVehV26aSDvj1E8dx40LFC6aO3PtHHb32qeF5cB44erMLOnQLJsdPl48UXX9TNN9+sJUuW6PPPP9f8+fN18sknN8631uqaa67RnDlztGnTJo0aNUozZ87UgAEDMpm73az3ueymy6S617aZGpXNPVkmebWMyQskV9hYv0I2da1U/YQkf+v0+OEyyZtkot2DihYq1nqym2dIFfdIqt46IzZQSt4kk7NfYNnC5m93P6c5l92v1FfljdN279NVF90xUYd+76AAk4XL0oXLdet/3aXPP17XOK0gma8Jvxqrky78DqU6Q1a/95mmj79dKxd/2DgtJxHT9y/4jn5y0zjFctyei9jpyy4VFRUaOnSo7rjjjmbnT58+XTNmzNCsWbP0+uuvq6CgQGPGjFF1dXWzy2eT9TfKfjVWqlu83RxPqp4vu/ECWes3+71oP2s92Y3n71A8JEm1r8puOEPWL2/2e7FzbOoGqWKmmhQPSar/QHbDmekzfOiwv855Vrf8ZGaT4iFJX/7rK1190jQt/tubASULl+V/X6Erjr9epZ+sbzK9oqxSd1x8t/7y2ycDShYu61d/ocmHX6kPljbdP9TV1OuRW/9Pt/x4pvNMO10+jj/+eF1//fU65ZRTdphnrdWtt96qK6+8UieddJKGDBmiP/7xj1q7dq0effTRTOTdOZUPSP46SV4zM/30aevaV1ynCp+a5xoKXnNFzpO8z6Sqh12nCh1bv1qqur+FuZ5ka2Q33+k0UxjVVtdq9i/ua3aetek/Z/1sruyWL/Bvm/OL++T7VtZvfizvveohVaQqHacKn3k3PaqKVKV8b8d9tLVWz97/oj5ctspppozecLpq1SqVlpZq9OjRjdOSyaQOOeQQvfrqq81+T01NjVKpVJNXptjKP6v5A+IWUdmqRzK2vl2VrZqv1jclv+F3gQ6pfkxSa/cbeFL1X2VtlatEofTGgjdVUdbyAc9aq9UrPtNHyz5xFyqE1n5UqhWvf9Bi8ZCk2po6vfTI6w5ThY/v+3p67gvy61s+FkZjET37x0UOU2W4fJSWlkqSevTo0WR6jx49Gudtr6SkRMlksvHVp0+fzAXyv2pjAU/y1rWxDNrkrVPrJU+S/6WTKGFmvS8ltXX9u17yy1zECa2NpZvaHmZJG0o3ZTtKqG1c1/Z2Go1G0r8P/NtqKmtUU1Xb6jLWShvWbXITqEHgH7WdOnWqysrKGl9r1qzJ3A+PtHWTY1SK9src+nZV0V5q/R25acfvAm1J37Tb1qn+HCmym4M04dW1uEvbwyyp2ze6ZD9MiHUt7tzmMl69r66Mc4ck8hPK65Tb6jLGSN2K3Y5zRstHz549JUnr1jU9m7Bu3brGedtLJBIqKipq8soUk3+aWv8rejJ5P8zY+nZV6TFs7r6abZbJP91NmDDLPUltXUZU7okypvUdDVp38PEHqLBLyx8/NBGj/oP7qv/gvg5ThU/Pft21/+EDFYm0fJoptyChw08Z4TBV+EQiEY059xhFYi0fC716X98ef7S7UMpw+ejfv7969uyphQsXNk5LpVJ6/fXXddhhh2VyVe2TP06K9lbz78qNlPiWFD/UdarwSRwlxQ9X85tTVIruKeWd5jpV6JhYb6ngxy3MjUqmQKbTBU4zhVFOPEcX3Hpus/NMxCgSMbrg1nP5CGgG/NdvzlEkFm2xgPzkprOU14nHIXTUjy47WcmuhYpEm9lHG+m7E0er//5uy/ROl4/Nmzdr2bJlWrZsmaT0TabLli3T6tWrZYzR5MmTdf311+vxxx/X8uXLdc4556i4uLjJs0BcMZEimS7zpMTRanoRNy7lnyOz2wx2IBlgTESm80wpb6yknG3mRKTEaJmuD8pEgnmQTdiYTpfKdPqZZLYbz5wDZLo+LBPj3XgmjD7rSP3yocnq1rtrk+nfGNBLNy64Ugccs39AycJl4IgB+s1z12qPQU3v9evcI6mf/eECnTTpOwElC5duxV30u1du0JCjmj4HKLdTrs668lRddOdPnGcydic/L/bCCy/omGOO2WH6+PHjde+99zY+ZGz27NnatGmTDj/8cN15553aZ5992vXzU6mUksmkysrKMnoJxnprpbp3JcWk+HCZSGHGfja2sv4mqfZNSfVSzmCZaPOX29Ax1lZLtf+QbIUU21smtlfQkULJ8zy9+/LKxiecfvPgvXnDkgXWWn345qrGJ5wOPmJfRWM8STYbPvvwc61avlrx3LgGH7mv8goyd5l2Z47fO10+si1b5QMAAGTPzhy/A/+0CwAA2LVQPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgVCzoAK5YWyt5qyXlSNE+MobelQ3WepK3RpLXMM7xoCOFkrVW8tdKfqUULZaJFAQdKbQ2rtukTV+k1LVXZxV1LQw6TmhZPyV5pVIkKRPtEXSc0KqurNG6T9YrnhdXz37dZYwJJEfoy4e11bKbb5cqH5JseXpipFjqdL6Ud0ZgAx821lqp8j7Zit9Lfml6oknK5o+T6XQBJSSDbPXTsptvk+pXNkyJy+adIlN4iUykS6DZwmTF6x/o7l8+qGXPvSNJMhGjkd8/WOfdeKb6DvxGwOnCw9avkS2/Rap5SpKXnpZzUHp7jo8INlyIbN5UoblXP6wFdz+nmsoaSVKfgcU668pT9a0zj3Cex1hrbSZ/oOd5uvbaa3X//fertLRUxcXFmjBhgq688sp2HehTqZSSyaTKyspUVFTUoSzW1spumCDVLZXk77hA/gRFiq7o0DqQLh42da1U9VAzcyNS/FCZznNkTI7jZOFjKx+STV0jyUja9j/dqBT9hkzXP8tEOgeULjyWPf+Opn7nevm+le9t3XdEohHlFiT0u5dvUL9BfQJMGA62frXsV6c2vDH0tpmTPjNtdrtTJvdbgWQLk4pUpSYffqVWr/isyfZsjGSt9JObztKPfnFSh9ezM8fvjF97mDZtmmbOnKnbb79dK1as0LRp0zR9+nTddtttmV5V26r+ItUtUbPFQ5Iq75Wte9dppFCqW9pC8ZAkX6p9Rap63GmkMLL+BtnUr7d8td1cT/I+k90803Ws0PF9Xzefd6c8z2+yo5Yk3/NVXVGjGZPmBJQuXGz5jc0UDym9z7ayZVPTl8zRIX+55YkdioeULh6S9IepD2j96i+cZsp4+XjllVd00kkn6YQTTlC/fv106qmn6rjjjtMbb7yR6VW1yVY+0MYSUdnKPznJEma2cp6kaCtLGNnKB13FCa+qR7XjTnpbnlT1Z3bWHbTsuXe0/tMvZP3mTwr7nq/lL67QZx9+7jhZuFjvC6nmebW8TVvJbmxYBv8ua62emPX0DsVjWyZi9Le73Y5zxsvHyJEjtXDhQr3//vuSpLfeeksvvfSSjj/++GaXr6mpUSqVavLKmPrV2vEd4rY8yfsoc+vbVdV/rNYPilbyPnEUJrxs/adqveRJshWSv9FJnrD67IP2lYq1H5ZmOUnIeWvU+v5ZkqJS/acu0oRWdUW1yr5o+7jqukxn/IbTyy+/XKlUSgMHDlQ0GpXnebrhhhs0bty4ZpcvKSnRddddl+kYaaZAsjWtLBCRTDI7696VRJLa8R6E7ZhOrtKEV6ST2t5Zm/R2j39bQTK/XcvlF7VvObTAtOeTQ37Ddo9/Vzw3rmgsIq++lTMfxii/MM9hqiyc+fjTn/6kBx54QA8++KCWLl2quXPn6je/+Y3mzp3b7PJTp05VWVlZ42vNmjWZC5P3fbX+TtGXyf1u5ta3izK5J6j1g2Kk4XeBjkhvq62dYYpI8SNk2Fl3yIjvHqic3NZvju5a3FkDD9nbUaKQiu0tRfsp/calJREp8W1HgcIpGovq8B8eqmis5cO9V+/p6B+NcpgqC+Xj0ksv1eWXX66xY8dq8ODBOvvss3XJJZeopKSk2eUTiYSKioqavDLF5J8jmVw1/9eMpjf+XDbsDss7QYruoeaLXlQynWTymz/zhfYzOYOkxDFqfns2koxMpwscpwqfTrsV6PSft16WJ/xqrKLRNi6BoVXGGJnCKWr5jYuR8s6Uie7uMlYonXH5KTKRiExkx6IXiUY0+Mh9NeSo/Zxmynj5qKysVCTS9MdGo1H5fsunfLLFxHrLdPmjFOneMCWmxgNkzmCZznN5/kQGGJMr0+U+KbZvw5SoGq/oRXvKdLlPJtozqHihYpL/IyVGN3y1zTibTjK73SETPzCoaKFyzrWn6/Sff1+RaHqHHcuJyhijWDym/75lvL5zHh//zAST+x2Zol9LSihdoGNKH5ZM+jlMRVMDzRcWew3tpxv/eoWKuqTPisZyoopE08fpA0cP1q8evcz5M68y/pyPCRMm6Nlnn9Vdd92lQYMG6c0339T555+v8847T9OmTWvz+zP5nI8trPWkmkWydW/JmJgUP1LKGcIDxjLMWivVLZWteVmSJxMflr4MYHiHmGm2/kOp+hlZWykTGyDljpExiaBjhc6Xazdo0cOvaNP6MnXv201Hjx2lws5c1so062+Wqv9P1luTfk5N7vEy0eKgY4VOXW2dXnl0sT566xMl8hI67PvDteeQPTL283fm+J3x8lFeXq6rrrpK8+fP1/r161VcXKwzzjhDV199teLxts8yZKN8AACA7Aq0fHQU5QMAgP88gT7hFAAAoDWUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4FQs6QLZZa6Wap2Qr/ijVvS2ZqBQ/UqbgPJn4sKDjhYqt/Ydsxd1SzUuSfCnnAJmCCVLiWBljgo4XGrb+I9mKe6TqBZKtlmJ7yuSPk/JOlTGh/0/amQ2lGzX/d3/VU3NfUPlX5epa3EXfnThaJ00ao4JkQdDxQqOqolpPznxaT971jNav/kIFuxVo9FlH6geTT1D3Pt2Cjhcavu/r2fte1KO3L9Cq5asVT+Ro1CkjdOqUE7XnkD2c5zHWWpvpH/rZZ5/psssu04IFC1RZWam9995b99xzj4YPH97m96ZSKSWTSZWVlamoqKhDOay1sqlfSVUPKH2Sx2+YE5XkyxSVyOT/oEPrQJqtnCebukbpcfYapkbT/7/gJzKdLqWAZICteVV240Slx3jLOBtJNl2qO98pY+LBBQyJzz78XJcccZXKviyX7/mN003EqPeAXvqfv/9ayW4d2z9Bqiir0M+OuVYfv/2prL/1UBSJRlRQlKdbXrhO/Qe7PzCGjed5uvHM3+nFP78qEzGNYx2NRSQZXfO/P9dhJ7Z9fG7Lzhy/M37ZZePGjRo1apRycnK0YMEC/fOf/9Qtt9yizp07Z3pVbat5uqF4SFuLh5TeaVvZ1BWy9f9ynytkbP3HDcXDausBUVv/f8XvpdoXA0gWLtZWyW66UFKdmo5zw0679u9SxT0BJAufknG/U+qrpsVDkqxv9dmHpbr9oj8ElCxc5lx2v1YtX92keEiS7/mqSFXpV6f/Vll4f7zL+evsZ/XiX16VpCZj7dX78j1PN5zxPyrfuNlppoyXj2nTpqlPnz665557NGLECPXv31/HHXec9tprr0yvqk224j61/lc0slUPu4oTWrZynlof52j6shc6pmqBZMvVWDZ2YGUr75O1fgvz0R4fLP1YKxd/JK+++XH0PV8v/vk1bVy3yW2wkKkoq9DTcxftUPC28D1f/1q5Vm+98K7jZOFirdUjv/u/VuZLtVV1euaPixymykL5ePzxxzV8+HCddtpp6t69u4YNG6Y5c+a0uHxNTY1SqVSTV8bUva2mZzy250l1yzK3vl1V3Ztq+k58e55U95arNKFl699Wm7dp+esl/0snecJq5eKP2lzG93x9uOyT7IcJsU9XfKa6mrpWl4lEI+36faBlNVW1+tf7n7f8nkXpy4nv/8PtOGe8fHz88ceaOXOmBgwYoKeeeko//elPddFFF2nu3LnNLl9SUqJkMtn46tOnT+bCtHnznZHE9fGOa8cYmpzsxwi9dm6rjHWHxOLtu2k3p53LoXntGT9rLePcQen7OlpnjBTLcTvOGS8fvu/rwAMP1I033qhhw4bp/PPP18SJEzVr1qxml586darKysoaX2vWrMlcmMQxSt/02DKTODpz69tFpcewtZtJo1LiW47ShJdJHCGpvpUlIlJskEwkgPurQuTA0YPbvDk6rzBXAw8Z4ChROPUf3FfJ3Vu/KdH6VsO/c4CbQCGVE8/R0KMHKRJt+XDv1fs6+Hi3n/7MePno1auX9ttvvybT9t13X61evbrZ5ROJhIqKipq8MsUUTFDL55oikklKeSdnbH27rPxTJdNJzW9O6Z24yT/HaaRQio+SogPUcqH2ZTqd7zJRKHXv003HnDGqxZ21MdIPLjpBufkJx8nCJZYT0+k//36L8yPRiEZ8d5j6DvyGw1Th9KPLTm7x3ppINKIe/XbXqJMPdpop4+Vj1KhRWrlyZZNp77//vvbYI4DPEecMlkn+Rumd9Za/qkm/TJFMl7tlIoXOc4WNiXSW6fwHyRSocXwlpcc8JrPbrTI53wwuYEgYE5HpMkeKbtkZb9mm02XEdLpEJvf4QLKFzeRZ52vIkek3UVtKyJbT10ePPVxnX3NaYNnC5NSfnajv/de3JW0d3y3jvc/wPXX5fRcFli1MDh5zgCb97jwZYxrHd8vZvS69Ouump65yftkl48/5WLx4sUaOHKnrrrtOp59+ut544w1NnDhRs2fP1rhx49r8/kw+52ML65XKVj7ccNNjjkzukVLuSTKRThn5+Uizfkqqmi9b87Kkepn4gVLe6TLR7kFHCxVra6Xqv8lWPy3ZzVJsH5n8H8nE3H+iLMx839eSZ97Wwvtf1IbSTerRt5u+8+Njtd9h+/DMmgxbufhDLfj9Qq39eJ2S3Qr1rTOP0IjvDlM02vplc+yctR+V6v9mP6uPlq1SPC+ukSeN0DFjRyqRl5mzeDtz/M7KQ8aefPJJTZ06VR988IH69++vKVOmaOLEie363myUDwAAkF2Bl4+OoHwAAPCfJ9AnnAIAALSG8gEAAJyifAAAAKcoHwAAwCnKBwAAcIryAQAAnKJ8AAAApygfAADAKcoHAABwivIBAACconwAAACnKB8AAMApygcAAHCK8gEAAJyifAAAAKcoHwAAwCnKBwAAcIryAQAAnKJ8AAAApygfAADAKcoHAABwivIBAACconwAAACnKB8AAMApygcAAHCK8gEAAJyifAAAAKcoHwAAwCnKBwAAcIryAQAAnKJ8AAAApygfAADAKcoHAABwivIBAACconwAAACnKB8AAMApygcAAHCK8gEAAJyifAAAAKcoHwAAwCnKBwAAcIryAQAAnKJ8AAAApygfAADAKcoHAABwivIBAACconwAAACnKB8AAMApygcAAHCK8gEAAJyifAAAAKcoHwAAwCnKBwAAcIryAQAAnKJ8AAAAp2JBB3DB1n8iWzlPqntTUkwmcZSUf6pMpEvQ0ULFel9KVX+Wrfm7JE+KHySTN1Ym1jfoaKFi/Qqp+gnZ6r9JtkKKfVMmf6xMzv5BRwuV+rp6vfLYYj1734vasG6TevbbXWPO/ZYO+vYQRSK8b8sUa63eeuFdLfjDQn32YamS3Qp17JlH6IhTD1VOPCfoeKGyavmnemLWM/pg6cfKzU9o1Mkj9O1zjlRBssB5FmOttdlcwU033aSpU6fq4osv1q233trm8qlUSslkUmVlZSoqKurw+m3ln2VTV0kykryGqUYyBTKdfy8TP7DD64Bka9+Q3ThRsjWS/IapUUlWJjldJu/7AaYLD1u/WnbDWZJfqvQ2bZUeZ08q+G+ZTpfIGBNsyBCoKKvQ1ONv0IrXPlAkGpHv+Y1/jjzpYF358CUcGDPA8zzdPOEOLXzg74rGIvLqfZmIkfWt+g/uq+nPXq3ddk8GHTMU5k17VH+Y+kDjOMtIRkbJboW6+blr1W9Qnw6vY2eO31mt74sXL9Zdd92lIUOGZHM1LbK1b8qmrlT6YOhtO0eylbIbfyLrbwokW5hY76tmioeUHnNftuwXsnXvBpQuPKz1ZDf+WPK/2DKl4c+GbbtillT9eBDRQuc3P5mplYs/kiT5nt/kz1ef+IfuvXJeYNnCZF7Jo1r44N8lKX1AlGT99Hb96T//pRvP/F1g2cLk1Sf+oT9MfUDS1nGWTZ91Sm3YrMvH/Fq1NXVOM2WtfGzevFnjxo3TnDlz1Llz52ytplW24h61/Ff006esqx5xGSmcqv7UTPHYlpGtmOsyUTjVvCh5n6ppkd6Wka2YoyyfzAy90k/W66VHXm8sG9uzvtVjdz6lqs1VjpOFS11tnf731ie3dujt+J6vNxcu16rln7oNFkIPT39UkWjzx0Lf8/XV2o36+19ec5opa+Vj0qRJOuGEEzR69OhWl6upqVEqlWryypjahnsPWmRla17K3Pp2UbbmRbVcPCTJSx840SG29mW1fpuWlerfl+xGV5FC6c2Fy1s8IG5RU1mj99740E2gkPr47dUq37C51WVMxGjps8sdJQqn2po6vfvyyhbLtCRFohEtXfi2w1RZuuF03rx5Wrp0qRYvXtzmsiUlJbruuuuyEUOtHxC3qM/SunclrRW8nVkGrWvnGFrGuiNa20lvq/H0Nf4t7RlnY4y8erbnjrB+e7dnt+Oc8TMfa9as0cUXX6wHHnhAubm5bS4/depUlZWVNb7WrFmTuTA5w5S+Ga8lESnnoMytb1eVc5BaH+eoFGecO8rkHKA2y3KklxTp6iJOaO176D5tLhONRbX3sH7ZDxNi/Qb1ViIv3uoyvudrv8Pa/n2gZYm8hPbYr3erN6L7vq/9Dv2mw1RZKB9LlizR+vXrdeCBByoWiykWi2nRokWaMWOGYrGYPK9pu0okEioqKmryyhSTP16tv1uMyOT/KGPr21WZ/DPU+nlqTyb/HFdxwiv3O5LprJb/szUyBeNlDB8D7Yg9h+yhQaMGKhprfhwj0Yi+debhfAqjg/I65en4Hx/b4r0IkVhE/fbvo0GjBjpOFj4/vOR7Ld4LZoxRXkGujj3rCKeZMr6XOvbYY7V8+XItW7as8TV8+HCNGzdOy5YtUzTa2jvkzDK5x0gFExu+2na9UUkRmeRvZKI9neUJKxPrK5O8SenNaftxlkyn/yeTGBlEtFAxJiHTeaZkEmo6zg3/GSe+LVHyMuKKBy5Sl16dZSLbvFs06R11v0F9dMGt5wYXLkTOKzlT3xyxd+PYbhGJRFTUpVDX/OXnfHQ8A8ace4yOG3+0JDUpe5FYRLF4VNc+cqkKivKdZsr6cz4k6eijj9YBBxwQyHM+pPQNkbbij1LdMkkxKXGMTME5Mjn7ZuTnI83WvStbcW/DzaUNDxnLH0/xyDBbv0a28j6p+q+SrZJie8vknyXlflfGuCv3YZfaUK4nZz2jp+55Tpu+SGn33l31vf86TmPOO0Z5BW1fUkb71FbX6ql7X9CTdz2t0lXrVdi5k44bf7ROvGCMOnfn7FKmWGv10iOv67E7FujDZZ8onsjRET88VKdc9F313qc4I+vYmeP3LlE+AABAdu3M8dvJ49VfeOEFF6sBAAD/AbgzDQAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTsaADuGBtrVT9tGzd25KiMokjpPhhMsYEHS1UrPWl2pdla16W5MvkHCDljpYx8aCjhY6tWy5b/bRkq2Rie0u535OJdAo6VujY+tVS9ROy/gaZaC8p9/sy0e5Bxwod62+Qqh6T9f4lmaRM3vdkYnsGHSt0rF8pVf9Vtv49ySRkEsdKOcMCORYaa63N5A8sKSnRI488ovfee095eXkaOXKkpk2bpm9+85vt+v5UKqVkMqmysjIVFRV1OI+tXSK78QLJbtTWrlUvxfaR6TwnvUNBh9n6NbIbz5e8j9RknCO7y+x2p0x8aJDxQsP65bKbLpJqX5YUlWQkeZJyZZIlMnnfDTZgSFjryaZ+LVU9pPQYRyT5kiTT6SKp4Ke8eckQWzFXtnya0ttxVJJN///ck2WS1/PmJUNs9fOyZT+T7Gal99EN45xzkEznO2QiXTq8jp05fmf8ssuiRYs0adIkvfbaa3rmmWdUV1en4447ThUVFZleVZts/aeyG86TbFnDlPqGl6T6j2Q3nCNra5znChvrV8puOFvyPmmYss04+1/Jbpwg660NKF14WGtlN/0/qfbVhime0uNsJVXLlk2RrX0juIAhYst/01A8rNKlo77hT192861S1YNBxgsNW/W4bPkN2rod1yu9XUuqfixdANFhtm657KZJkt1yHN5mnOuWyW48P33m2qGMl4+//e1vmjBhggYNGqShQ4fq3nvv1erVq7VkyZJMr6pNtvJeSbXa8o6lKU/yPpWq/+Y2VBhVPyH5a9W4MTfhS7ZatuI+16nCp+5tqfYVNb89W0lGdvOdjkOFj/U3SZV/VHpMW1hm8+2yts5ZpjCytqHItbyEVPVnWa/UVaTQspvvUnp7bm6b9hr2La82My97sn7DaVlZ+qxDly7Nn9KpqalRKpVq8sqYqr+q+QPiFhFZykeH2eoFSp+aboknVT/pKk5o2Zqn1PptWp5U+4qsX+4qUjjVLJLURrHwv5Lq3nISJ7TqP5C8f7WxkJWqn3USJ6ysrZdqFqr1Y2HU+bEwq+XD931NnjxZo0aN0v7779/sMiUlJUomk42vPn36ZC6ArWwroWTZUXeY3azW3iWml2nrd4E2+e0cQ8a6Y9o7foxzx7Rr/CKMc0fZWrVePCTJOh/nrJaPSZMm6Z133tG8efNaXGbq1KkqKytrfK1ZsyZzAWJ7qvV35FEpNiBz69tVRQcofaNYSyJSbG9XaUIrffd/GzsRUyhl4MaxXVq0nZ+yiPbPbo6wi/ZV24cgT4rt5SJNeJk8KbJ724s5HueslY8LL7xQTz75pJ5//nn17t27xeUSiYSKioqavDLF5I9T6+/IPZm8sRlb367K5I9V6wdFv+F3gQ7JO0lSTisLRKT8H8mY1pZBm+Ij2jgwRqX4SJlYBs/S7oJMtKuU+LZafuNipEg3KXGUy1ihY4yRyT9LbR7u837gJM8WGS8f1lpdeOGFmj9/vp577jn17x/gu4O8U6T4Edrx7EfD1wUXyOS07yPAaJmJD5Xyf7zlq+3nSoljpdwTXMcKHRNJyiR/ra0f/dxWVIrtKVPw0wCShYsxRiY5Xemit/2BMSqZQpmia90HCyFTdEXDmbpmxllRmeTNMmaXeBxVdhVMkGKDtON+I/21KfylTLSn00gZLx+TJk3S/fffrwcffFCFhYUqLS1VaWmpqqqqMr2qNhmTI9N5pkyni6VI160zonvJJKcrUjjZeaawMoW/kCm6UYrusXViZHeZTj+T2e02GdPaZRm0l8k7Rabz76WcA7aZWCDlnyPTZZ5MpDCwbGFi4gfKdP2TlDhGWwt1LP0wt67/KxPrF2C68DDRXjJd/1fKPUXSlud5GCk+SqbLQzKJUUHGCw1j8mS63CcV/EQy21xdiA1KP4ep4Gz3mTL9kLGWHrxzzz33aMKECW1+f6YfMraFtZ7kr5cUkyLdeEBQllhrJf8LSZ4U6U7pyCLrb5BsVbrk8SCmrLF+ueSXSZEuMpH8oOOElrVVkveVFCmSiWRu34+mrK1r2Ecn0pe+Mmhnjt8ZP5+V4S6TMcZEJZ5mmnXGGInHTzuRiScSom0mUihxRinrjMmTYi3fH4jMMCZHihYHHYN/WA4AALhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATlE+AACAU7GgA7hi6z+W6pZLikrxQ2Wi3YKOFErWWy/VviHJk3KGysT6BR0plKy/Wap9WbKVUmxvKba/jDFBxwoda2ul2lckf4MU6SXFR8iYaNCxQsdaX6pbInlrJLOblBglYxJBxwolW7dCqn9PMrlSfKRMJBlIjqyVjzvuuEM333yzSktLNXToUN12220aMWJEtlbXIut9LrvpMqnutW2mRmVzT5ZJXi1j8pxnCiPrV8imrpWqn5Dkb50eP1wmeZNMtHtQ0ULFWk928wyp4h5J1VtnxAZKyZtkcvYLLFvY2Mq/yJZPl+ymrRMjvaSia2VyjwksV9jYmldkU1eli8cWplDqdLGUfzalOkNs/UfpY2H929tMjcvmj5Mp/LmMyXGaJyuXXR5++GFNmTJF11xzjZYuXaqhQ4dqzJgxWr9+fTZW1yLrb5T9aqxUt3i7OZ5UPV924wXpxo0OsdaT3Xj+DsVDklT7quyGM2T98kCyhY1N3SBVzFST4iFJ9R/IbjgzfYYPHWYrH5ZNXdG0eEiSXyq76b9la14MJFfY2NrFsht/LHn/2m5GuWz59VLl3cEECxnrrZX96gyp/t3t5tRKlffKll3hPFNWysdvf/tbTZw4Ueeee672228/zZo1S/n5+br7bscbUuUDkr9OktfMTD992rr2FbeZwqjmuYaC11yR8yTvM6nqYdepQsfWr5aq7m9hrifZGtnNdzrNFEbW1qTPeDQ/N/2/qRJZa92FCqn0OFttGdcd59+avsSIDrGbZ0u2XM0fC61U/Zhs3T+dZsp4+aitrdWSJUs0evTorSuJRDR69Gi9+uqrOyxfU1OjVCrV5JUptvLPav6AuEVUtuqRjK1vV2Wr5qv1Tclv+F2gQ6ofk9Ta/QaeVP1XWVvlKlE41Sxq2FG3xEreR1L9CmeRwsjWr5bq3lLr++gaqfppV5FCyVpfqnpEzRePLaKyVY86SpSW8fLx5ZdfyvM89ejRo8n0Hj16qLS0dIflS0pKlEwmG199+vTJXBj/qzYW8CRvXebWt6vy1qn1HYgk/0snUcLMel9Kauv6d73kl7mIE17t3Vb9L7KbI+zaNc5RxrmjbJV2uEy740LO99GBf9R26tSpKisra3ytWbOm7W9qr0hbNzlGpWivzK1vVxXtpdbfkZt2/C7QlvRNu22d6s+RIrs5SBNi7d1WIz3aXgYta9c4e1KUce4QkyeZ/LYWcr49Z7x8dOvWTdFoVOvWNT2jsG7dOvXs2XOH5ROJhIqKipq8MsXkn6bW/4qeTN4PM7a+XVV6DFs7pSeZ/NPdhAmz3JPU1mVE5Z4oY3JdJQqnxJHpj3u2yEixfaTYN10lCiUT6y3lHKRW99EmT0p821mmMDImIuWdqrYu2Zq8U1xFkpSF8hGPx3XQQQdp4cKFjdN839fChQt12GGHZXp1rcsfJ0V7q/lBN1LiW1L8ULeZwihxlBQ/XM1vTlEpuqeUd5rrVKFjYr2lgh+3MDcqmQKZThc4zRRGxsRlin7Z0lxJEZnCK/kIaAaYwsuV3j83fygynS6ViRQ4zRRGpmCiFOmsFgtI3o9kcvZxmikrl12mTJmiOXPmaO7cuVqxYoV++tOfqqKiQueee242VtciEymS6TJPShytptfK41L+OTK7zWAHkgHGRGQ6z5Tyxkra9rPiESkxWqbrgzKRTkHFCxXT6VKZTj+TzHbjmXOATNeHZWJ9gwkWMibvJJnk/0iR7c7WRvvJdP6DTII3LZlg4kNlutyXflDetiLdZIpKZArOCiZYyJhoD5kuD0vxg7ebkS8VTJIputZ9Jpulz4vdfvvtjQ8ZO+CAAzRjxgwdcsghbX5fKpVSMplUWVlZRi/BWG+tVPeupJgUHy4TKczYz8ZW1t8k1b4pqV7KGSwT3fFSGzrO2mqp9h+SrZBie8vE9go6UihZ60l1SxuecNpTyhnCG5YssNZK9f9MP+/DJNP7aLPLPIDbKVv/qVS/UjIJKedgmUhb94O0384cv7NWPv5d2SofAAAge3bm+B34p10AAMCuhfIBAACconwAAACnKB8AAMApygcAAHCK8gEAAJyifAAAAKcoHwAAwCnKBwAAcOpr9/zaLQ9cTaVSAScBAADtteW43Z4Hp3/tykd5ebkkqU+fPgEnAQAAO6u8vFzJZLLVZb52/7aL7/tau3atCgsLM/4POKVSKfXp00dr1qzh343JIsbZDcbZDcbZHcbajWyNs7VW5eXlKi4uViTS+l0dX7szH5FIRL17987qOoqKitiwHWCc3WCc3WCc3WGs3cjGOLd1xmMLbjgFAABOUT4AAIBTu1T5SCQSuuaaa5RIJIKOEmqMsxuMsxuMszuMtRtfh3H+2t1wCgAAwm2XOvMBAACCR/kAAABOUT4AAIBTlA8AAODULlM+7rjjDvXr10+5ubk65JBD9MYbbwQdKXRKSkp08MEHq7CwUN27d9fJJ5+slStXBh0r9G666SYZYzR58uSgo4TOZ599prPOOktdu3ZVXl6eBg8erH/84x9BxwoVz/N01VVXqX///srLy9Nee+2lX//61+3690HQuhdffFEnnniiiouLZYzRo48+2mS+tVZXX321evXqpby8PI0ePVoffPCBk2y7RPl4+OGHNWXKFF1zzTVaunSphg4dqjFjxmj9+vVBRwuVRYsWadKkSXrttdf0zDPPqK6uTscdd5wqKiqCjhZaixcv1l133aUhQ4YEHSV0Nm7cqFGjRiknJ0cLFizQP//5T91yyy3q3Llz0NFCZdq0aZo5c6Zuv/12rVixQtOmTdP06dN12223BR3tP15FRYWGDh2qO+64o9n506dP14wZMzRr1iy9/vrrKigo0JgxY1RdXZ39cHYXMGLECDtp0qTGrz3Ps8XFxbakpCTAVOG3fv16K8kuWrQo6CihVF5ebgcMGGCfeeYZe9RRR9mLL7446Eihctlll9nDDz886Bihd8IJJ9jzzjuvybQf/OAHdty4cQElCidJdv78+Y1f+75ve/bsaW+++ebGaZs2bbKJRMI+9NBDWc8T+jMftbW1WrJkiUaPHt04LRKJaPTo0Xr11VcDTBZ+ZWVlkqQuXboEnCScJk2apBNOOKHJto3MefzxxzV8+HCddtpp6t69u4YNG6Y5c+YEHSt0Ro4cqYULF+r999+XJL311lt66aWXdPzxxwecLNxWrVql0tLSJvuPZDKpQw45xMmx8Wv3D8tl2pdffinP89SjR48m03v06KH33nsvoFTh5/u+Jk+erFGjRmn//fcPOk7ozJs3T0uXLtXixYuDjhJaH3/8sWbOnKkpU6boiiuu0OLFi3XRRRcpHo9r/PjxQccLjcsvv1ypVEoDBw5UNBqV53m64YYbNG7cuKCjhVppaakkNXts3DIvm0JfPhCMSZMm6Z133tFLL70UdJTQWbNmjS6++GI988wzys3NDTpOaPm+r+HDh+vGG2+UJA0bNkzvvPOOZs2aRfnIoD/96U964IEH9OCDD2rQoEFatmyZJk+erOLiYsY5xEJ/2aVbt26KRqNat25dk+nr1q1Tz549A0oVbhdeeKGefPJJPf/88+rdu3fQcUJnyZIlWr9+vQ488EDFYjHFYjEtWrRIM2bMUCwWk+d5QUcMhV69emm//fZrMm3ffffV6tWrA0oUTpdeeqkuv/xyjR07VoMHD9bZZ5+tSy65RCUlJUFHC7Utx7+gjo2hLx/xeFwHHXSQFi5c2DjN930tXLhQhx12WIDJwsdaqwsvvFDz58/Xc889p/79+wcdKZSOPfZYLV++XMuWLWt8DR8+XOPGjdOyZcsUjUaDjhgKo0aN2uGj4u+//7722GOPgBKFU2VlpSKRpoeiaDQq3/cDSrRr6N+/v3r27Nnk2JhKpfT66687OTbuEpddpkyZovHjx2v48OEaMWKEbr31VlVUVOjcc88NOlqoTJo0SQ8++KAee+wxFRYWNl43TCaTysvLCzhdeBQWFu5wH01BQYG6du3K/TUZdMkll2jkyJG68cYbdfrpp+uNN97Q7NmzNXv27KCjhcqJJ56oG264QX379tWgQYP05ptv6re//a3OO++8oKP9x9u8ebM+/PDDxq9XrVqlZcuWqUuXLurbt68mT56s66+/XgMGDFD//v111VVXqbi4WCeffHL2w2X98zRfE7fddpvt27evjcfjdsSIEfa1114LOlLoSGr2dc899wQdLfT4qG12PPHEE3b//fe3iUTCDhw40M6ePTvoSKGTSqXsxRdfbPv27Wtzc3PtnnvuaX/5y1/ampqaoKP9x3v++eeb3SePHz/eWpv+uO1VV11le/ToYROJhD322GPtypUrnWQz1vIYOQAA4E7o7/kAAABfL5QPAADgFOUDAAA4RfkAAABOUT4AAIBTlA8AAOAU5QMAADhF+QAAAE5RPgAAgFOUDwAA4BTlAwAAOEX5AAAATv1/WNAh/mnfuJsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating a test dataset\n",
    "# really does help to narrow down where bugs are festering\n",
    "dataset = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 2, 1],\n",
    "    [0, 4, 1],\n",
    "    [0, 6, 1],\n",
    "    [0, 8, 1],\n",
    "    [0, 10, 1],\n",
    "    \n",
    "    \n",
    "    [2, 0, 1],\n",
    "    [2, 2, 1],\n",
    "    [2, 4, 1],\n",
    "    [2, 6, 1],\n",
    "    [2, 8, 1],\n",
    "    [2, 10, 1],\n",
    "\n",
    "    [4, 0, 1],\n",
    "    [4, 2, 1],\n",
    "    [4, 4, 1],\n",
    "    [4, 6, 1],\n",
    "    [4, 8, 1],\n",
    "    [4, 10, 1],\n",
    "    \n",
    "    [6, 0, 1],\n",
    "    [6, 2, 1],\n",
    "    [6, 4, 0],\n",
    "    [6, 6, 0],\n",
    "    [6, 8, 0],\n",
    "    [6, 10, 0],\n",
    "    \n",
    "    [8, 0, 1],\n",
    "    [8, 2, 1],\n",
    "    [8, 4, 0],\n",
    "    [8, 6, 0],\n",
    "    # [8, 8, 0],\n",
    "    [8, 8, 1], # testing max depth. this point adds like 4 more splits\n",
    "    [8, 10, 0],\n",
    "    \n",
    "    \n",
    "    [10, 0, 1],\n",
    "    [10, 2, 1],\n",
    "    [10, 4, 0],\n",
    "    [10, 6, 0],\n",
    "    [10, 8, 0],\n",
    "    [10, 10, 0],\n",
    "    \n",
    "])\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "\n",
    "# all_feature_split_rules = find_splits(X)\n",
    "# best_split = find_best_split(X, y, all_feature_split_rules)\n",
    "# print(best_split)\n",
    "\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=dataset[:, 2], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i guess that you have to run this recursively\n",
    "# base caeses. pure node or max depth reached. \n",
    "# for a given set of data, that could've already been split\n",
    "# calculate the next best split\n",
    "\n",
    "# so first go through and build out the splits without worrying about tracking things for prediction\n",
    "\n",
    "# verbose for debugging\n",
    "\n",
    "max_depth = 4\n",
    "def build_tree(X: np.ndarray, y: np.ndarray, current_depth: int = 1):\n",
    "    \n",
    "    if current_depth > max_depth: # max depth exceeded\n",
    "        return\n",
    "    if len(np.unique(y)) == 1: # pure node\n",
    "        return\n",
    "    \n",
    "    index_col, threshold = find_best_split(X, y, find_splits(X))\n",
    "    print(index_col, threshold)\n",
    "    X_left = X[X[:, index_col] <= threshold]\n",
    "    y_left = y[X[:, index_col] <= threshold]\n",
    "    X_right = X[X[:, index_col] > threshold]\n",
    "    y_right = y[X[:, index_col] > threshold]\n",
    "    \n",
    "    build_tree(X_left, y_left, current_depth + 1)\n",
    "    build_tree(X_right, y_right, current_depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0\n",
      "1 3.0\n",
      "1 7.0\n",
      "1 9.0\n",
      "0 7.0\n",
      "0 9.0\n"
     ]
    }
   ],
   "source": [
    "root = build_tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0 7.0\n",
      "1 3.0 8\n",
      "1 7.0 8\n",
      "1 9.0 8\n",
      "0 7.0 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.predict([7.0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so all of the right computations are being performed. \n",
    "# now i need to store the results so that they can be used for prediction\n",
    "# creating nodes that use feature and threshold\n",
    "# nodes need left and right child nodes\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index_col: int, threshold: float, left: \"Node\", right: \"Node\", majority_class):\n",
    "        self.feature_index_col = feature_index_col\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.majority_class = majority_class # if a node is cut off due to max depth, this becomes useful. also works for purity base case. \n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.feature_index_col is None or self.threshold is None: # check leaf node\n",
    "            return self.majority_class\n",
    "        else:\n",
    "            print(self.feature_index_col, self.threshold, x[self.feature_index_col])\n",
    "            if x[self.feature_index_col] <= self.threshold:\n",
    "                return self.left.predict(x)\n",
    "            else:\n",
    "                return self.right.predict(x)\n",
    "\n",
    "max_depth = 7\n",
    "def build_tree(X: np.ndarray, y: np.ndarray, current_depth: int = 1):\n",
    "    \n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    majority_class = values[np.argmax(counts)]\n",
    "    if current_depth > max_depth or len(np.unique(y)) == 1: # max depth exceeded or pure node\n",
    "        return Node(None, None, None, None, majority_class)\n",
    "    \n",
    "    index_col, threshold = find_best_split(X, y, find_splits(X))\n",
    "    print(index_col, threshold)\n",
    "    X_left = X[X[:, index_col] <= threshold]\n",
    "    y_left = y[X[:, index_col] <= threshold]\n",
    "    X_right = X[X[:, index_col] > threshold]\n",
    "    y_right = y[X[:, index_col] > threshold]\n",
    "    \n",
    "    # last node created is the root (dfs)\n",
    "    left_node = build_tree(X_left, y_left, current_depth + 1)\n",
    "    right_node = build_tree(X_right, y_right, current_depth + 1)\n",
    "    \n",
    "    return Node(index_col, threshold, left_node, right_node, majority_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing putting it all together\n",
    "dtree = DecisionTree()\n",
    "dtree.fit(X, y)\n",
    "dtree.predict([[7.0, 8]]), dtree.predict([[7.1, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine.drop(columns=[\"quality\"]).to_numpy()\n",
    "y = wine[\"quality\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8lOQ1vd-gGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMzJB5Ot-gG2",
    "outputId": "c939a2b6-1f8d-4e5b-f013-e3afb125ad9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x134f5d550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Error should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Error should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.621875"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_test).mean()\n",
    "# guessing they mean accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSgiIwTc-gHB"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([34, 27,  9, 10,  5, 23]), array([1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample_indices = np.random.choice(X.shape[0], int(np.sqrt(X.shape[0])), replace = False)\n",
    "feature_sample_indices = np.random.choice(X.shape[1], int(np.sqrt(X.shape[1])), replace = False)\n",
    "data_sample_indices, feature_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = 1000, size_allowed = 1):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        np.random.seed(42)\n",
    "        for idx in range(self.n_trees):\n",
    "            X_subset = X\n",
    "            y_subset = y\n",
    "            \n",
    "            if self.n_features == 'sqrt':\n",
    "                feature_sample_indices = np.random.choice(X.shape[1], int(np.sqrt(X.shape[1])), replace = False)\n",
    "                X_subset = X[:, feature_sample_indices]\n",
    "                y_subset = y\n",
    "            \n",
    "            if self.n_split == 'sqrt':\n",
    "                data_sample_indices = np.random.choice(X.shape[0], int(np.sqrt(X.shape[0])), replace = False)\n",
    "                X_subset = X_subset[data_sample_indices]\n",
    "                y_subset = y_subset[data_sample_indices]\n",
    "            \n",
    "            print(idx, X_subset.shape, y_subset.shape)\n",
    "            \n",
    "            temp_clf = DecisionTree(max_depth=self.max_depth, min_samples_split=self.size_allowed)\n",
    "            temp_clf.fit(X_subset, y_subset)\n",
    "            self.trees.append((temp_clf, feature_sample_indices))\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def predict(self, dataset: np.ndarray):\n",
    "        result = []\n",
    "        for datapoint in dataset:\n",
    "            tree_predictions = []\n",
    "            for tree, feature_sample_indices in self.trees:\n",
    "                datapoint_subset = datapoint[feature_sample_indices]\n",
    "                tree_predictions.append(tree.predict([datapoint_subset])[0])\n",
    "            mode_prediction, _ = mode(tree_predictions)\n",
    "            result.append(mode_prediction)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4vXXHnP-gHD"
   },
   "outputs": [],
   "source": [
    "# calculate mode of numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bg4eRJ58-gHF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([2, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1279, 3) (1279,)\n",
      "1 (1279, 3) (1279,)\n",
      "2 (1279, 3) (1279,)\n",
      "3 (1279, 3) (1279,)\n",
      "4 (1279, 3) (1279,)\n",
      "5 (1279, 3) (1279,)\n",
      "6 (1279, 3) (1279,)\n",
      "7 (1279, 3) (1279,)\n",
      "8 (1279, 3) (1279,)\n",
      "9 (1279, 3) (1279,)\n",
      "10 (1279, 3) (1279,)\n",
      "11 (1279, 3) (1279,)\n",
      "12 (1279, 3) (1279,)\n",
      "13 (1279, 3) (1279,)\n",
      "14 (1279, 3) (1279,)\n",
      "15 (1279, 3) (1279,)\n",
      "16 (1279, 3) (1279,)\n",
      "17 (1279, 3) (1279,)\n",
      "18 (1279, 3) (1279,)\n",
      "19 (1279, 3) (1279,)\n",
      "20 (1279, 3) (1279,)\n",
      "21 (1279, 3) (1279,)\n",
      "22 (1279, 3) (1279,)\n",
      "23 (1279, 3) (1279,)\n",
      "24 (1279, 3) (1279,)\n",
      "25 (1279, 3) (1279,)\n",
      "26 (1279, 3) (1279,)\n",
      "27 (1279, 3) (1279,)\n",
      "28 (1279, 3) (1279,)\n",
      "29 (1279, 3) (1279,)\n",
      "30 (1279, 3) (1279,)\n",
      "31 (1279, 3) (1279,)\n",
      "32 (1279, 3) (1279,)\n",
      "33 (1279, 3) (1279,)\n",
      "34 (1279, 3) (1279,)\n",
      "35 (1279, 3) (1279,)\n",
      "36 (1279, 3) (1279,)\n",
      "37 (1279, 3) (1279,)\n",
      "38 (1279, 3) (1279,)\n",
      "39 (1279, 3) (1279,)\n",
      "40 (1279, 3) (1279,)\n",
      "41 (1279, 3) (1279,)\n",
      "42 (1279, 3) (1279,)\n",
      "43 (1279, 3) (1279,)\n",
      "44 (1279, 3) (1279,)\n",
      "45 (1279, 3) (1279,)\n",
      "46 (1279, 3) (1279,)\n",
      "47 (1279, 3) (1279,)\n",
      "48 (1279, 3) (1279,)\n",
      "49 (1279, 3) (1279,)\n",
      "50 (1279, 3) (1279,)\n",
      "51 (1279, 3) (1279,)\n",
      "52 (1279, 3) (1279,)\n",
      "53 (1279, 3) (1279,)\n",
      "54 (1279, 3) (1279,)\n",
      "55 (1279, 3) (1279,)\n",
      "56 (1279, 3) (1279,)\n",
      "57 (1279, 3) (1279,)\n",
      "58 (1279, 3) (1279,)\n",
      "59 (1279, 3) (1279,)\n",
      "60 (1279, 3) (1279,)\n",
      "61 (1279, 3) (1279,)\n",
      "62 (1279, 3) (1279,)\n",
      "63 (1279, 3) (1279,)\n",
      "64 (1279, 3) (1279,)\n",
      "65 (1279, 3) (1279,)\n",
      "66 (1279, 3) (1279,)\n",
      "67 (1279, 3) (1279,)\n",
      "68 (1279, 3) (1279,)\n",
      "69 (1279, 3) (1279,)\n",
      "70 (1279, 3) (1279,)\n",
      "71 (1279, 3) (1279,)\n",
      "72 (1279, 3) (1279,)\n",
      "73 (1279, 3) (1279,)\n",
      "74 (1279, 3) (1279,)\n",
      "75 (1279, 3) (1279,)\n",
      "76 (1279, 3) (1279,)\n",
      "77 (1279, 3) (1279,)\n",
      "78 (1279, 3) (1279,)\n",
      "79 (1279, 3) (1279,)\n",
      "80 (1279, 3) (1279,)\n",
      "81 (1279, 3) (1279,)\n",
      "82 (1279, 3) (1279,)\n",
      "83 (1279, 3) (1279,)\n",
      "84 (1279, 3) (1279,)\n",
      "85 (1279, 3) (1279,)\n",
      "86 (1279, 3) (1279,)\n",
      "87 (1279, 3) (1279,)\n",
      "88 (1279, 3) (1279,)\n",
      "89 (1279, 3) (1279,)\n",
      "90 (1279, 3) (1279,)\n",
      "91 (1279, 3) (1279,)\n",
      "92 (1279, 3) (1279,)\n",
      "93 (1279, 3) (1279,)\n",
      "94 (1279, 3) (1279,)\n",
      "95 (1279, 3) (1279,)\n",
      "96 (1279, 3) (1279,)\n",
      "97 (1279, 3) (1279,)\n",
      "98 (1279, 3) (1279,)\n",
      "99 (1279, 3) (1279,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x1372f3050>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = RandomForest(n_trees= 100, n_split=\"sqrt\")\n",
    "clf = RandomForest(n_trees=100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUwAMpuT-gHK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.684375"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "(pred == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
